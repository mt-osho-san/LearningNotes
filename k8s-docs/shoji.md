# 概要

## Kubernets のコンポーネント

- k8s はコントロールプレーンのコンポーネントとノードというマシン郡で構成されている。

### コントロールプレーンコンポーネント

- コントロールプレーンコンポーネントはシンプルにするために、すべてのコントロールプレーンコンポーネントを同じマシンで起動し、そのマシンではユーザーコンテナは実行しない
- kube-apiserver
  - k8s api を外部に提供する k8s のコントロールプレーンのコンポーネントで、コントロールプレーンのフロントエンドになる
- etcd
  - 一貫性、高可用性を持ったキーバリューストアで、k8s のすべてのクラスター情報の保存場所として利用される
- kube-scheduler
  - コントロールプレーン上で動作するコンポーネントで、新しく作られた Pod にノードが割り当てられているか監視し、割り当てられていなかった場合にその Pod を実行するノードを選択
  - Pod のリソース要求量やハード、ソフトの制約、アフィニティ、有効期限などを考慮して行われる
- kube-controller-manager
  - 下記の複数プロセスを実行
    - ノードコントローラー：ノードがダウンした場合の通知と対応を担当
    - Job コントローラー：単発タスクを表す Job オブジェクトを監視し、そのタスクを実行して完了させるための Pod を作成 (単発タスクはどういうものがあるのか)
    - EndpointSlice コントローラ：EndpointSlice オブジェクトを作成 (Service と Pod を紐づける)
    - ServiceAccount コントローラー：新規の名前空間に対してデフォルトの ServiceAccount を作成する
- cloud-controller-manager
  - クラウド上で実行する際に利用するもので、クラウドプロバイダー固有のコントローラーのみを実行する

### ノードコンポーネント

ノードコンポーネントはすべてのノードで実行され、稼働中の Pod 管理や k8s の実行環境を提供する。

- kubelet
  - クラスター内の各ノードで実行されるエージェントで、各コンテナが Pod で実行されていることを保証する
  - PodSpec のセットを取得し、それらの PodSpec に記述されているコンテナが正常に実行されている状態を保証する
- kube-proxy
  - kube-proxy はクラスター内の各 node で動作しているネットワークプロキシ
- コンテナランタイム
  - コンテナの実行を担当するソフトウェアで、Docker、containerd, CRI-O, すべての k8s CRI をサポート

### アドオン

アドオンはクラスターレベルの機能を提供しているので、アドオンのリソースで名前空間が必要なものは kube-system 名前空間に属する

- DNS
  - クラスター DNS 以外のアドオンは必須ではないが、すべての k8s クラスターはクラスター DNS を持つべき
  - クラスター DNS は環境内の他の DNS サーバーに加えて、k8s サービスの DNS レコードを提供する DNS サーバー
- WebUI
  - クラスター内で実行されているアプリケーションについて管理、トラブルシューティングができる Web ベース UI
- コンテナリソースの監視
- クラスターレベルのロギング

---

## Kubernetes API

k8s 内のオブジェクトの状態をクエリで操作できる。
k8s のコントロールプレーンの中核は API サーバーとそれが公開する HTTP API
ユーザー、クラスターのさまざまな部分、および外部コンポーネントはすべて API サーバーを介して互いに通信する。

### 永続性

- k8s は API リソースの観点からシリアル化された状態を etcd に書き込むことで保存する

### API の変更

k8s は既存のクライアントとの互換性を破壊しない、互換性を一定期間維持して、他のプロジェクトが適応する機会を提供する

---

# k8s オブジェクトを理解する

k8s は永続的なエンティティで、下記を表現可能

- どのようなコンテナ化されたアプリケーションが稼働しているか
- それらのアプリケーションが利用可能の名リソース
- アプリケーションがどのように振る舞うかのポリシー、再起動、アップグレードなど
  k8s は意図の記録で、一度オブジェクトを作成すると、k8s は常にそのオブジェクトが存在し続けるように動く

### オブジェクトの spec(仕様)と status(状態)

ほとんどの k8s オブジェクトはオブジェクトの設定を管理する 2 つの入れ子になったオブジェクトのフィールドを持っている (spec, status)
spec を持っているオブジェクトは作成時に spec を設定する必要があり、望ましい状態としてオブジェクトに持たせたい特徴を記述する必要がある
status オブジェクトは、オブジェクトの現在の状態を示し、k8s のコントロールプレーンは望ましい状態と status が一致するように管理をする

- 必須フィールド
  - apiVersion
  - kind: どの種類のオブジェクトを作成するのか
  - metadata: オブジェクトを一意に特定するための情報
  - spec: 望ましい状態

---

# Kubernetes オブジェクト管理

### 管理手法

- 命令型コマンド
  - 現行のオブジェクトを対象にし、開発用プロジェクトで利用される
- 命令型オブジェクト設定
  - 個々のファイルを対象にし、本番用で使われる
- 宣言型オブジェクト設定
  - ファイルのディレクトリを対象にし、本番用で使われる

---

# オブジェクトの名前と ID

クラスター内のオブジェクトにはそのタイプのリソースに固有の名前があり、クラスター全体で一位の UID もある。
同じ名前空間内に myapp-1234 という名前の Pod は 1 つしか含められないが、myappp-1234 という名前の 1 つの Pod と 1 つの Deployment を含めることができる。
一意でない属性の付与のためにはラベルとアノテーションがある

## 名前

クライアントから提供され、オブジェクトを参照する文字列 (ex. /api/v1/pods/<名前>)
同じ種類のオブジェクトでは同じ名前を同時には持てないが、削除すればできる
以下３つはよく使われる命名規則

- DNS サブドメイン名
- DNS ラベル名 => ドメイン名の . で区切られた部分のこと
- パスセグメント名

## UID

オブジェクトを一意に識別するための k8s が生成する文字列 k8s クラスターの生存期間中にわたって生成されたすべてのオブジェクトは異なる UID を持っている

---

# ラベルとセレクター

ラベルはオブジェクトに割り当てられるキーとバリューのペア
ラベルはオブジェクトをグルーピングするために使うことができ、いつでも追加修正が可能

## ラベルセレクター

ラベルセレクターを介して、クライアントとユーザーはオブジェクトのセットを指定可能。
k8s API がサポートするセレクターは等価ベースと集合ベース

- 等価ベース
  - =,==,!=
- 集合ベース
  - in, notin, exists

---

# 名前空間

k8s は同一の物理クラスター上で複数の仮想クラスターの動作をサポートするが、この仮想クラスターを NameSpace と呼ぶ

## 複数の NS を使うとき

複数のチーム、プロジェクトにまたがる多くのユーザーがいる環境での試用を目的としている。
数人から数十人のクラスターに対して NS を作成する必要はない
同じアプリケーションの異なるバージョンなど、少し違うリソースをただ分割するだけに複数の NS を使う必要はない。
=> この場合は NS 内でリソースを区別するためにラベルを使う

## NS を利用する

`kube get namespace` によってクラスター内の NS の一覧を表示できる
k8s の起動時には 4 つの初期 NS が作成されている

- default: 他に NS を持っていないオブジェクトのためのデフォルト NS
- kube-system: k8s システムによって作成されたオブジェクトのための NS
- kube-public: すべてのユーザーから読み取り可能
- kube-node-lease: クラスターのスケールに応じたノードハートビートのパフォーマンスを向上させる各ノードに関連した Lease オブジェクトのための NS
- NS の設定
  `kubectl run nginx --image=nginx --namespace=<insert-namespace-name-here>`

## すべてのオブジェクトが NS に属しているとは限らない

Node や PersistentVolume のような低レベルのリソースはどの NS にも属さない

```
# NS に属しているものを確認するコマンド
kubectl api-resources --namespaced=true
```

---

# アノテーション

ユーザーは識別用途でない任意のメタデータをオブジェクトに割り当てるためにアノテーションを使用できる
アノテーションはラベルと同様にキーバリューのマップとなる
下記はアノテーションの利用例

- ビルド、リリースやタイムスタンプのようなイメージの情報
- ロギング、管理分析用のポインタ
- デバッグ目的で使用されるためのクライアントライブラリやツールの情報

---

# フィールドセレクター

１つ以上のリソースフィールドの値を元に k8s リソースを選択するためのもの。
status.phase が running の Pod をすべて選択している例: kubectl get pods --field-selector status.phase=Running

---

# ファイナライザー

削除対象としてマークされたリソースを削除する前に特定の条件が満たされる魔 dk8s を待機させるための名前空間付きキー
Pod 削除時に PersistentVolume を開放する必要があり、ファイナライザーが PV の解放を担当し、解放が完了するまで Pod は削除されない
=> 整合性を担保している

## どのようにファイナライザーは動作するか

マニフェストファイルを使ってリソースを作るときに metadata.finalizers フィールドで指定できる

---

# オーナーと従属

いくつかのオブジェクトは他のオブジェクトのオーナーになっている。
例えば ReplicaSet は Pod の集合のオーナー

## オーナーリファレンス

従属オブジェクトはオーナーオブジェクトを参照するためのフィールドを持っている。
従属オブジェクトはオーナーオブジェクトが削除されたときにガベージコレクションをブロックするかどうかを管理する ownerReference.blockOwnerDeletion フィールドも持っている

---

# 推奨ラベル

共通のラベルを使うことで、様々なツールに対応できるようになるというもの
例としては、app.kubernetes.io/name や app.kubernetes.io/instance などがある。

---

# ノード

ノード上で実行される Pod にコンテナを配置してワークロードの実行をする。
ノードは 1 つの VM または物理的なマシン
1 つのノード上のコンポーネントには kubelet, コンテナランタイム、kube-proxy が含まれる (すべてのノードに？)

## 管理

ノードを API サーバーに加えるには２つの方法がある (クラスターに加えるということ？)

1. ノード上の kubelet がコントロールプレーンに自己登録する

- kubelet のフラグ --register-node が true のとき、自身を登録しようとする。これは推奨されているパターン

2. ユーザーが手動で Node オブジェクトを追加

- 上記フラグを false にする。

## ノードのステータス

- Address
  - HostName,ExternalIP,InternalIP などがある
- Conditions
  - Ready: ノードの状態が有効で Pod を配置可能な場合に True になる
  - DiskPressure: ディスク容量が圧迫されているときに True になる
  - PIDPressure: プロセスが圧迫されているときに True になる
- Capacity と Allocatable
- Info: カーネルや k8s,Docker のバージョンなど

## ノードのハートビート

ハートビートは k8s ノードから送信され、ノードが利用可能化判断するのに役立つ

1. Node の .status 更新
2. Lease object?

## ノードコントローラー

ノードの存続期間中に下記役割を果たす

1. ノードが登録されたときに CIDR ブロックをノードに割り当てる
2. ノードコントローラーの内部ノードリストをクラウドの利用可能なマシンと一致させる
3. ノードの状態を監視

# ノードとコントロールプレーン間の通信

信頼できないネットワーク上でクラスターが実行できるようにしている。

## ノードからコントロールプレーンへの通信

ノードからのすべての API は API サーバーで終了し、コントロールプレーンコンポーネントはどれもリモートサービスを公開するようにはなっていない。
API サーバーは 1 つ以上の形式のクライアント認証が有効になっている状態で、セキュアな HTTPS ポートでリモート接続をリッスンするように設定されている
ノードは、有効なクライアント認証情報とともに、API サーバーに安全に接続できるように、クラスターのパブリックルート証明書でプロビジョニングされる必要がある
これらにより、ノード上で動作する Pod からコントロールプレーンへの接続はデフォルトでセキュアで、信頼されていないネットワークやパブリックネットワークを介して実行可能

## コントロールプレーンからノードへの通信

2 つの経路がある

1. API サーバーからクラスター内の各ノードで実行される kubelet プロセスへの通信経路

- Pod のログの取得、実行中の Pod へのアタッチ、kubelet のポート転送機能の提供に使われる
- kubelet の HTTPS エンドポイントで終了する接続だが、デフォルトでは kubelet サービング証明書を検証しないので、信頼できないネットワークを解するのは安全じゃない
- よって、サービング証明書を検証するか、SSH トンネルを使うなどする (SSH トンネルは現在は非推奨で、Konnectivity を使うべし)

2. API サーバーのプロキシ機能を介した API サーバーから任意のノード、Pod、サービスへの通信経路

- デフォルトで HTTP なので、認証も暗号化もされない。
  => 何に使う？

## konnectivity サービス

SSH トンネルの代替として、コントロールプレーンからクラスターへの通信に TCP レベルのプロキシを提供

# リース

共有リソースをロックしたり、ノード間の活動を調整する機構
k8s においては、coordination.k8s.io API グループの Lease オブジェクトに表されており、ハートビートやコンポーネントレベルのリーダー選出といったシステムに利用されている

## ハートビート

各ノードに Lease オブジェクトがあり、それに対して Update リクエストを送り、spec.renewTime フィールドを更新する
コントロールプレーンはこのフィールドのタイムスタンプを見て、Node が利用可能か判断する

## リーダー選出

あるコンポーネントのインスタンスが常にひとつだけ実行されていることを保証するのにも利用される

## API サーバーのアイデンティティー

kube-apiserver は LeaseAPI を利用して、自身のアイデンティティーをその他のシステムに向けて、公開するようになった
これは kube-apiserver が何台のコントロールプレーンを稼働させているのかをクライアントが知るためのメカニズムを提供する
kube-apiserver 間の調整が必要な場合に使える

# コントローラー

クラスターの状態を監視し、必要に応じて変更を加えたり、要求したりする制御ループ

## コントローラーパターン

k8s のオブジェクトには目的の状態を表す spec フィールドがあり、この値に近づける責務をコントローラーが持つ

## API サーバー経由でコントロール

Job コントローラーが新しいタスクを見つけると、その処理が完了するように、kubelet が正しい数の Pod を実行することを保証する
しかし、Job コントローラーは自分自身で Pod やコンテナを実行することはなく、API サーバーに依頼をする
Job では目的の状態となると完了し、Finished マークが付く

## 直接的なコントロール

Job とは対象的にクラスターの外部に変更を加える必要があるコントローラーもある
例えば、クラスターに十分な数の Node が存在することを保証する制御ループの場合、そのコントローラーは必要に応じて新しい Node をセットアップするために、外部とやりとりをする
コントローラーが目的の状況を実現するために変更を加えてから、現在の状態をクラスターの API サーバーに報告する
エアコンの例では、非常に寒いときは別のコントローラーが霜防止ヒーターをオンにするようなケース

# クラウドコントローラーマネージャー

クラウド特有の制御ロジックを組みこむコントロールプレーンのコンポーネント
コンポーネント間の密なつながりが不要な設計なので、ハイブリッドでも利用可能

## クラウドコントローラーマネージャーの機能

### ノードコントローラー

クラウドインフラで新たなサーバーが作成されたときに、Node オブジェクトを作成する責務を持つ

### ルートコントローラー

クラスター内の異なるノード上で稼働しているコンテナが相互に通信できるようにクラウド内のルートを適切に設定する責務を持つ

### サービスコントローラー

マネージドロードバランサー、IP アドレスネットワークパケットフィルタやヘルスチェックのようなクラウドインフラコンポーネントのインテグレーションを行う

# cgroup v2 について

Linux ではコントロールグループがプロセスに割り当てられるリソースを制限している
kubelet と基盤となるコンテナランタイムは cgroup をインターフェースとして接続する必要がある

## cgroup v2 とは

v1 からの改善点

- 単一階層設計の API
- より安全なコンテナへのサブツリーの移譲
- 強化されたリソース割り当て管理と複数リソース間の隔離

## cgroup v2 を使う

おすすめの方法はデフォルトで v2 が有効で使える Linux ディストリビューションを使うこと

# コンテナランタイムインターフェース

クラスターコンポーネントを再コンパイルせずに kubelet がさまざまなコンテナランタイムを使用できるようにするプラグインインターフェース

## API

kubelet は gRPC を介してコンテナランタイムに接続するときにクライアントとして機能する
コンテナランタイムは --image-service-endpoint コマンドラインフラグで kubelet 内で個別に設定可能

# ガベージコレクション

k8s がリソースをクリーンアップするために使用するさまざまなメカニズムの総称

## カスケード削除

k8s は owner reference がなくなったオブジェクトをチェックして削除する
カスケード削除ではオブジェクトの依存関係を自動的に削除するかを制御可能であり、下記 2 つの種類がある

### フォアグラウンドカスケード削除

- 削除するオーナーオブジェクトは最初に削除進行中になる
- API サーバーはオブジェクトの metadata.deletionTimeStamp フィールドをオブジェクトに削除のマークが付けられた時刻に設定する
- API サーバーは metadata.finalizers フィールドを foregroundDeletion に設定
- オブジェクトは削除プロセスが完了するまで、k8s API を介して表示されたままになる

### バックグラウンドカスケード削除

- API サーバーがオーナーオブジェクトをすぐに削除し、コントローラーがバックグラウンドで依存オブジェクトをクリーンアップする
- これはデフォルト

## 未使用のコンテナとイメージのガベージコレクション

kubelet は未使用のイメージに対して 5 分ごとに未使用のコンテナに対して 1 分ごとにガベージコレクションを実行する
ガベージコレクションの時間などは設定ファイルで調整可能

# Mixed version proxy

複数のバージョンを併用して利用する場合に、バージョンの歳による予期しない 404 エラーなどを防ぐ

---

# コンテナ

アプリケーションとランタイムの依存関係を一緒にパッケージ化できる

# イメージ

コンテナイメージはアプリケーションと依存関係のあるソフトウェアをすべてカプセル化したバイナリデータ

## イメージの名称

レジストリのホスト名やポート番号を含むこともある (レジストリのホスト名を指定しない場合は Docker パブリックレジストリを意味しているとみなす)
タグを追記して、バージョンを識別することもある

## イメージの更新

デフォルトでは Pod 内のコンテナの Pull ポリシーは明示的に指定されていない場合、IfNotPresent に設定される
イメージがすでに存在する場合、このポリシーは Kubelet にイメージの Pull をスキップさせる

### イメージ Pull ポリシー

kubelet が指定されたイメージを Pull しようとするときに影響するもの

- IfNotPresent
  - イメージがローカルにない場合のみイメージが Pull される
- Always
  - コンテナを起動するときは毎回レジストリに照会する
  - ローカルにキャッシュされ r た同一ダイジェストのイメージがあったらそれを利用
- Never

  - イメージを取得しようとせずに、ローカルにイメージがある場合のみコンテナを起動する

- Pod がいつも同じバージョンのコンテナイメージを使用するために、イメージのダイジェストを指定することもできる => <image-name>:<tag> ではなく <image-name>@<digest> を利用
- 同一のイメージタグで、そのコードを変更すると、新旧のコードが混在する可能性があるが、ダイジェストは一意なので、混在を避けるときには有効

## イメージインデックスを使ったマルチアーキテクチャイメージ

コンテナのアーキテクチャ固有バージョンに関する複数のイメージマニフェストを指すことができる

## プライベートレジストリ

認証の方法

- プライベートレジストリへの認証を Node に設定する
- 事前に Pull されたイメージ
- Pod で ImagePullSecrets を指定する
- ベンダー固有または、ローカルエクステンション

ユースケースによって、プライベートレジストリの認証方法やパブリックレジストリとの併用などもある

# コンテナ環境

k8s はコンテナにいくつかのリソースを提供(イメージとファイルシステム、コンテナ自体の情報、クラスター内のオブジェクトの情報)

## コンテナ情報

コンテナのホスト名はコンテナが実行されている Pod の名前で hostname で呼び出し可能

## クラスター情報

コンテナの作成時に実行されていたすべてのサービスのリストは環境変数として使用できる

# ランタイムクラス

RuntimeClass はコンテナラインタイムの設定を選択する機能で、コンテナラインタイム設定は Pod のコンテナを稼働させるために使われる

## RuntimeClass を使う動機

- 異なる Pod に異なる RuntimeClass を設定することで、パフォーマンスとセキュリティのバランスをとることができる。
- ワークロードの一部に高レベルの情報セキュリティ保証が必要な場合、ハードウェア仮想化を使用するコンテナランタイムで実行されるようにそれらの Pod をスケジュールすることを選択できる
- RuntimeClass を使って、コンテナランタイムは同じで設定が異なる Pod を実行することもできる。

## セットアップ

手順

- ノード上で CRI 実装を設定
- 対応する RuntimeClass を作成

## スケジューリング

scheduling フィールドを指定することで、設定された RuntimeClass をサポートするノードに Pod がスケジューリングされるように制限ができる

# コンテナライフサイクルフック

フックにより、コンテナは管理ライフサイクル内のイベントを認識し、対応するライフサイクルフックが実行されたときにハンドラーに実装されたコードを実行できる

## コンテナフック

コンテナに公開されている 2 つのフックがある

- PostStart
  - コンテナが作成された直後に実行される
- PreStop
  - API からの要求、liveness/startup probe の失敗、プリエンプションなどの管理イベントが原因でコンテナが終了する直前に呼び出される

---

# ワークロード

ワークロードとは、k8s 上で実行中のアプリケーションであり、下記の種類がある

- Deployment,ReplicaSet
- StatefulSet
- DaemonSet
- Job と CronJob

# Pod

Pod は k8s 内で作成、管理できる最小のデプロイ可能なユニットで、1 つまたは複数のコンテナのグループであり、ストレージやネットワークなどの共有リソースを持つ

## Pod とはなにか

Pod の共有コンテキストは、namespaces,cgroups などの隔離技術を用いて作られる

## Pod を使用する

下記のようなコマンドで実行できる

```
kubectl apply -f https://k8s.io/examples/pods/simple-pod.yaml
```

### Pod を管理するためのワークロードリソース

単一のコンテナしかもたないシングルトンの Pod でも直接作成するのではなく、Deployment や Job などのワークロードリソースを使用して作成する
Pod が状態を保持する必要がある場合は StatefulSet リソースを使用する
k8s の Pod は主に次の２つの方法で使われる

- 単一のコンテナを稼働させる Pod: 一般的なユースケース
- 強調して稼働させる必要がある複数のコンテナを稼働させる Pod

### Pod が複数のコンテナを管理する方法

単一の Pod 内の複数のコンテナはクラスター内の同じ物理または仮想マシン上で自動的に同じ場所に配置される
コンテナ間ではリソースや依存関係を共有したりできる

## Pod を利用する

Pod を直接つくることがないのは、Pod は一時的で、使い捨てできる存在として設計されたものであるから。

### Pod OS

.spec.os.name で windows か linux かを設定できる
ノードの OS がこれと異なる場合には Pod の実行は拒否される。

### Pod とコンテナコントローラー

ワークロードリソースは複数の Pod を作成、管理することができ、複製やロールアウト、障害時の自動回復を行う

## Pod の更新と取り替え

ワークロードリソースの Pod テンプレートが変更されると、既存の Pod を更新したり、パッチ適用するのではなく、更新されたテンプレートに基づいて新しい Pod を作成する
しかし、patch, replace などのアップデート操作には一部制限がある

- namespace, uid, creationTimeStamp は変更できない
- metadata.deletionTimestamp がある場合、metadata.finalizers に新しい項目を追加することはできない
  など

## リソースの共有と通信

Pod はデータの共有と構成するコンテナ間での通信を可能にする。

### Pod 内のストレージ

Pod 内のすべてのコンテナは共有ボリュームにアクセスできるので、複数のコンテナでデータを共有できる
Pod 内の１つのコンテナで再起動するときも、Pod 内の永続化データを保持できる

### Pod ネットワーク

各 Pod にはユニークな IP アドレスが割り当て割れる
Pod の中では localhost を使用して他のコンテナと通信できる

## コンテナの特権モード

Pod 内のどのコンテナも privileged フラグを設定することで、特権モードを有効にできる
OS の管理者権限が必要な場合に使える

## static Pod

API サーバーには管理されない、特定のノード上で kubelet デーモンに直接管理される Pod のこと
static pod は常に特定のノード上の 1 つの kubelet に紐づけられ、主な用途は、kubelet を使用して、個別のコントロールプレーンコンポーネントを管理すること

## コンテナの Probe

kubelet がコンテナに対して行うヘルスチェック

# Pod のライフサイクル

Pending => Running => Suuceeded or Failed
Pod は生存期間に一回だけスケジューリングされ、Pod は停止 or 終了までその Node 上で実行される

## Pod のライフタイム

アプリケーションコンテナと同様に Pod は比較的短期間の存在。
ノードが停止した場合、その Node にスケジュールされた Pod はタイムアウト時間の経過後に削除される
特定の Pod は新しいノードに再スケジュールされずに、同じ名前で新しい UID をもつ同一の Pod に置き換える

## Pod のフェーズ

取りうる値

- Pending, Running,Succeeded, Failed, Unknown

## コンテナのステータス

Pod のフェーズと同様に k8s は pod 内の各コンテナの状態を追跡する。取りうる値は下記。

- waiting
- Running
- Terminated

## コンテナの再起動ポリシー

Pod の spec には、Always,OnFailure,Never のいずれかの値を持つ restartPolicy フィールドがあり、デフォルトは Always

## Pod の Condition

Pod には、PodStatus があり、Pod が成功したかどうかなどの下記 PodCondition が含まれている
type, status, lastProbeTime, lastTransitionTime, reason, message

## コンテナの Probe

probe を行うためには、kubelet はコンテナ内でコードを実行するか、ネットワークリクエストする。

### チェックのメカニズム

4 つの異なる方法があり、各 probe はどれか 1 つを定義する必要がある

- exec
  - コンテナ内で特定のコマンドを実行
- grpc
- httpGet
- tcpSocket
  - ポートが空いていれば OK

### Probe の結果

Success, Failure, Unknown

### Probe の種類

- liveness Probe
  - コンテナが動いているかを示す
- readiness Probe
  - コンテナがリクエスト応答する準備ができているかを示す
- startupProbe
  - コンテナ内のアプリケーションが起動したかどうかを示す

## Pod の終了

Pod は不要になったときに、正常に終了できることが重要

### Pod の強制削除

デフォルトではすべての削除は 30s 以内に正常に行われるが、独自の値を指定可能で、0 にすると、強制削除になる

### 終了した Pod のガベージコレクション

失敗した Pod は人間またはコントローラーが明示的に削除するまで残る
コントロールプレーンは終了状態の Pod の数が閾値を超えたときに、それらの Pod を削除する

# Init コンテナ

アプリケーションコンテナの前に実行される特別なコンテナ
アプリケーションコンテナのイメージに存在しないセットアップスクリプトやユーティリティーを含めることができる

## Init コンテナを理解する

Init コンテナは下記を除いて通常のコンテナと全く同じ

- Init コンテナは常に完了するまで稼働
- 各 Init コンテナは次の Init コンテナが稼働する前に正常に完了する必要がある

Init コンテナが失敗すると何度も再起動するが、restartPolicy が Never ならば、その Pod 全体が失敗したということになる

### 通常のコンテナとの違い

Pod の準備ができる前に完了するので、lifecycle,livenessProbe,readinessProbe, startupProbe はサポートしない

## Init コンテナを使用する

ユースケース

- シェルコマンドを使ってサービスが作成されるのを待機

```
for i in {1..100}; do sleep 1; if nslookup myservice; then exit 0; fi; done; exit 1
```

- コンテナ起動を待機 sleep 60
- git リポジトリを Volume にクローン

# サイドカーコンテナ

メインのアプリケーションコンテナと同じ Pod で実行されるセカンダリーコンテナ
主要なアプリケーションコードを直接変更することなく、ロギング、モニタリング、セキュリティなどの機能を提供する

## サイドカーコンテナの有効化

SidecarContainers というフィーチャーゲートにより、指定可能
同じポッド内の他の init コンテナやアプリケーションコンテナとは独立しており、再起動などは独立して行える

# Disruption

## 自発的な Disruption と非自発的な Disruption

非自発的な Disruption

- ハードウェア障害
- 管理者が誤って削除
- カーネルパニック
- ノードのリソース不足による Pod の退避

自発的な Disruption

- Deployment やその他の Pod を管理するコントローラーの削除
- 再起動を伴う Deployment 内の Pod のテンプレートの更新
- Pod の直接削除

## Disruption への対応

非自発的な Disruption に対応する方法の例

- Pod が必要なリソースを要求するようにする
- 高可用が必要な場合はアプリケーションをレプリケートする
- レプリケートされたアプリケーションを実行する際にさらに高い可用性を得るにはラックを横断などでアプリケーションを分散させる

## Pod Disruption Budget

自発的な Disruption が頻発する場合でも可用性の高いアプリケーションの運用を支援する機能を提供している
各アプリケーションに対して PDB を作成できる。
これは、レプリカを持っているアプリケーションのうち、自発的な Disruption によって同時にダウンする Pod の数を制限する
例えば kubectl drain で指定した最低数よりも Pod が少なくなる場合にはドレインはブロックされる。

## クラスターオーナーとアプリケーションオーナーロールの分離

クラスターとアプリケーションのオーナーは別のロールであると考えると良い

- 多くのアプリケーションチームで k8s クラスターを共有している
- クラスター管理を自動化するためにサードパーティのツールやサービスを使用している場合

## クラスターで破壊的なアクションを実行する方法

ノードやシステムソフトウェアのアップグレードなどで、破壊的なアクションを実行する必要がある場合、次のような選択肢がある

- アップグレードの間のダウンタイムを許容
- もう一つの完全なレプリカクラスターにフェールオーバーする
- Disruption に体制のあるアプリケーションを作成し、PDB を使用

# エフェメラルコンテナ

すでに存在する Pod 内で一時的に実行するコンテナ
アプリケーションの構築ではなく、サービスの調査のために利用する

## エフェメラルコンテナを理解する

トラブルシューティングのためにすでに存在する Pod の状態を調査する必要が出てくることがある
このとき、既存の Pod 内でエフェメラルコンテナを実行することで、Pod の状態を調査したり、コマンドを実行したりする

### エフェメラルコンテナとはなにか

他のコンテナとは異なり、リソースや実行が保証されず、自動的に再起動されることもないので、アプリケーション構築には適さない

# Pod Quality of Service Classes

ノード上のリソースが不足した際に、どの Pod を優先的に維持し、どの Pod を停止するかを決定するのに使われる

## Quality of Service classes

使用可能なクラスは、Guaranteed,Burstalbe,BestEffort がある。

## Guaranteed

最も厳しいリソース制限があり、排除される可能性が最も低い

### Criteria

ポッド内のすべてのコンテナにはメモリ制限とメモリ要求が必要
ポッドのすべｔねおコンテナについて、メモリ制限はメモリ要求と等しい必要がある
ポッド内のすべてのコンテナには CPU 制限と CPU 要求が必要
ポッド内のすべてのコンテナの CPU 制限は CPU 要求と等しくなければならない

## Burstable

下限リソース保証があるが、特定の制限を必要としない Pod

### Criteria

Pod は Garantedd の基準を満たしていない
ポッド内の少なくとも１つのコンテナにメモリか CPU 要求か制限がある

## BestEffort

他の QoS クラスの Pod に割り当てられていないノードリソースを使用できる

### Criteria

上記 2 つの QoS クラスを満たさない場合

# ユーザー名前空間

ホストのユーザーとコンテナ内プロセスが利用するユーザーを隔離するもの
これにより、コンテナ内で root として稼働するプロセスをホスト側の異なるユーザーとして実行できる
ホストや他の Pod に危害を及ぼす、侵害されたコンテナによる被害を軽減することができる

# Downward API

K8s が Pod やコンテナに関する情報を内部で動作しているアプリケーションに提供する仕組み
Pod やコンテナに関するメタデータやリソース情報をアプリケーションが動的に取得できる

---

# Deployment

Deployment で理想状態を記述すると、Deployment コントローラーは指定された頻度で状態を変更する

## ユースケース例

- ReplicaSet をロールアウト
- より多くの負荷をさばけるように Deployment をスケールアップ

## 古いリビジョンのクリーンアップポリシー

Deployment が管理する古い ReplicaSet を自動で削除する .spec.revisionHistoryLimit というフィールドがある。

## Deployment Spec の記述

- Pod テンプレート
  - .spec.template は Pod テンプレートで、apiVersion や kind がないこと以外は Pod と同じスキーマ
- セレクター
  - .spec.selector は必須フィールドで、Deployment によって対象とされる Pod のラベルセレクターを指定

# ReplicaSet

ReplicaSet の目的は、どのようなときでも安定したレプリカ Pod のセットを維持することであり、レプリカ数の Pod が指定した数利用可能であることを保証するものとして使われる。

## ReplicaSet がどのように動くか

ReplicaSet が対象とする Pod をどう特定するかを示すセレクターや稼働させたい Pod のレプリカ数、テンプレートなどとともに定義される
Pod と連携は、metadata.ownerReferences というフィールドを使って行われる

## ReplicaSet をつかうとき

Deployment はより上位の概念で、Pod のアップデート機能などの有益な機能があるので、ReplicaSet を直接使うよりも、Deployment の利用が推奨
=> 使うことはほとんどない？
シンプルな機能のみで良い場合は ReplicaSet のほうが管理コストが少ない

# StatefulSet

ステートフルなアプリケーションを管理するための API
Pod のデプロイとスケーリングを管理し、Pod の順序と一意性を保証する

## StatefulSet のユースケース

- データの永続化が必要な場合
- 特定の起動順序が必要な場合
- 一意のネットワーク ID が必要な場合
- 分散アプリケーションの構築

## Pod アイデンティティー

StatefulSet の Pod は順序を示す番号、安定したネットワークアイデンティティー、安定したストレージからなる一意なアイデンティティーを持つ。

## デプロイとスケーリングの保証

- Pod がデプロイされるとき、それらの Pod は 0,1,2... の番号で順番に作成される
- Pod が削除される時、降順に削除される
- Pod にスケーリングオプションが適用される前に、その Pod の前の順番のすべての Pod が Runnnig かつ Ready になっている必要がある
- Pod が停止される前に、その Pod の番号よりも大きい番号のすべての Pod はシャットダウンしている必要がある

# DaemonSet

ノード上に Pod が 1 つ確実に実行していることを保証するためのリソース
ノードがクラスターに追加される時、Pod が Node 上に追加され、Node がクラスターから削除されたとき、GC により除去される
ユースケースは、Node

## Daemon Pod がどのようにスケジューリングされるか

DaemonSet コントローラーは対象となる各ノードに対して Pod を作成し、作成されるとデフォルトのスケジューラーが引き継ぐ

## Daemon Pod とのコミュニケーション

- push: DaemonSet 内の Pod は他のサービスに対して情報を送信する
- NodeIP、Known port: Pod が NodeIP を介して疎通できるようにするために、DaemonSet 内の Pod は hostport を使用できる
- DNS: endpoints リソースで DaemonSet を探すか、DNS から複数の A レコードを取得
- Service: 同じ Pod セレクターを持つサービスを作成し、いずれかのノード上の Daemon に疎通させるためにそのサービスを使う

# Job

一時的な処理を行うためのリソースで、タスクが完了すると終了する。
スケジュールに沿って job を実行したい場合は Cronjob を使う。

## 並列実行

job に適したタスク

- 非並列 job
- 固定の完了数をもつ並列 job
  - .spec.completions の数の Pod が成功すると完了
- ワークキューを利用した並列 job
  - 他の Pod が終了したかも確認できる

## Pod 失敗ポリシー

.spec.podFailurePolicy フィールドで定義される Pod 失敗ポリシーを使用するとコンテナの終了コードと Pod の条件に基づいてクラスターが Pod の失敗を処理できるようになる
例

- Pod の 1 つがバグを示す終了コードで失敗すると、Job をすぐに終了
- 中断が発生しても Job を完了するように Pod の失敗を無視

## Job の終了とクリーンアップ

Job が完了しても Pod は削除されない
Job を終了させるには、COMMANDO を実行するか、.spec.activeDeadlineSeconds に秒数を設定すること

# 終了したリソースのための TTL コントローラー

TTL コントローラーは実行を終えたリソースのライフタイムを制御するメカニズムがある

## TTL コントローラー

現時点では Job のみサポートしており、リソースが終了したあと指定した TTL の秒数後に削除するようになる

## 注意点

- 一度 TTL 期限がきれると、TTL を伸ばせないことがある
- クラスター内の時間差で誤った時刻に動作する可能性があるので NTP 稼働を必須とする

# Cronjob

繰り返しのスケジュールによって Job を作成する
一度のスケジュール実行であり、2 つのジョブが作成される、または１つも作成されない等の場合がないとは言えないので、ジョブは冪等であるべき

# ReplicationController

水平スケーリングでリソース管理できるレガシーな API で Deployment と ReplicaSet に置き換えられた。

# サービス、負荷分散とネットワーキング

k8s のネットワーキングは 4 つのことをする

- Pod 内のコンテナは、ループバック経由の通信を行う
- クラスターネットワーキングは異なる Pod 間の通信を提供
- サービスリソースは Pod で動作しているアプリケーションへクラスター外部から到達可能にする
- サービスでクラスター内部のみで使用するサービスの公開も可能

# サービス

クラスター内で 1 つ以上の Pod として実行されているネットワークアプリケーションを公開する方法

## サービスを利用する動機

動的にスケールしたり、IP アドレスが変化する中で、アプリケーションやコンポーネント間の通信を効率的かつ安定して行うための抽象化を提供

## サービスリソース

フロントエンドはバックエンドがなんであっても気にしなくても良い。
つまり、サービスによる抽象化で、クライアントからバックエンドの Pod の管理する責務を分離することを可能にする

### クラウドネイティブのサービスディスカバリー

アプリケーション内でサービスディスカバリーのために k8s API が使える場合、ユーザーはエンドポイントを API サーバーに問い合わせることができ、サービス内の Pod が変更されたときは更新されたエンドポイントの情報を取得できる
非ネイティブなアプリケーションのために、k8s はアプリケーションとバックエンド Pod の間でネットワークポートや LB を配置する方法を提供

### セレクターなしのサービス

- 本番では外部の DB クラスターを利用し、テスト環境では自身のクラスターが持つ DB を利用したい場合
- 異なる NS のサービスや他のクラスターのサービスに向ける場合
- ワークロードを k8s に移行する時、アプリケーションに対する処理をしながらバックエンドの一部を k8s で実行する場合
  上記の場合は Pod セレクターなしでサービスを定義できる=> なぜ？

### エンドポイントスライス

エンドポイントを複数のスライスに分割するものであり、100 個のエンドポイントに達すると、いっぱいであるとみなされ、追加のエンドポイントスライスが作成される

## 仮想 IP とサービスプロキシ

k8s クラスターの各 Node は kube-proxy を稼働させており、kube-proxy は ExternalName タイプ以外の Service ように仮想 IP を実装する責務がある

### なぜ DNS ラウンドロビンを使わないか

- DNS だと TTL をうまく扱わず、期限切れの後も名前解決の結果をキャッシュする
- いくつかのアプリでは、一度だけ DNS ルックアップを行い、その結果を無期限にキャッシュする

### プロキシモード

- user-space モードでは、kube-proxy がパケットを受取転送するが、iptables プロキシモードでは、kube-proxy が iptables を設定し、そのテーブルに則り、kube-proxy は経由せずに通信。
- IPVS (IP Virtual Server) を利用したもので、kube-proxy は IPVS のルールをカーネルに設定し、カーネル内で最適な Pod へパケットを直接転送。Pod からのレスポンスも IPVS 経由

## サービスディスカバリー

サービスオブジェクトを見つけ出すために環境変数と DNS という 2 つの主要なモードをサポートしている
環境変数は、Pod がノード上で稼働するときに、kubelet がアクティブなサービスに対して追加する。(IP,ポートの情報)

---

# ボリューム

コンテナでのファイルは一時的なものなので、下記問題がある

1. コンテナがクラッシュしたときにファイルが失われる
2. Pod で実行されているコンテナ間でファイルを共有する時に問題

## 背景

Docker のボリュームは厳密ではなく、管理が不十分
そこで、k8s では、Pod は永続ボリュームやエフェメラルボリュームなど任意のボリュームタイプを同時に使用できる

## ボリュームの種類

- awsElasticBlockStore
  - Pod を削除してもコンテンツは保持されたままアンマウントされる
- azureDisk
  - Azure データディスクを Pod にマウント
- azureFile
- cephfs
  - ceph は分散ストレージシステムであり、それを用いた分散ファイルシステム
- cinder
- emptyDir
  - Pod がノードに割り当てられたときに最初に作成され、その Pod がそのノードで実行している限り存在する
  - 最初は空
  - コンテナ間でのデータ共有や一時ファイルの作成などで使われる
  - Pod がノードから削除されると、emptyDir 内のデータは永久に削除される
- ## fc (fibre channel)
- gcePersistentDisk
- glusterfs
  - オープンソースの NFS ボリュームを Pod にマウントできるようにするもの
- iscsi
  - 既存の iSCSI ボリュームを Pod にマウントできる
  - 複数のコンシューマーから同時に読み取り専用としてマウントできる
- local
  - ノードのローカルストレージ
  - レイテンシが低いが、ノードへ依存し、耐障害性が低い
- nfs
- persistentVolumeClaim
- portworxVolume

### 投影ボリューム

複数のボリュームソースを 1 つのマウントに統合できる

- secret
  - パスワードなどの情報を Pod にわたすために使われる
  - tempfs によってバックアップされるので、不揮発ストレージに保存されることはない
- vsphereVolume

## subPath の使用

1 つの Pod で複数の用途に使用するために 1 つのボリュームを共有すると便利なこともある
ボリュームの一部のみをコンテナにマウントできる
volumeMounts[*].subPath でサブパスを指定

# 永続ボリューム

PV (PersistentVolume) はクラスターリソースの一部だが、Pod とは独立したライフサイクルを持っている
PVC (PersistentVolumeClaim) はユーザーによって要求されるストレージで、任意の PV のリソースを要求して作成される

## ボリュームと要求のライフサイクル

クラスターが PV を作成し、Pod が PVC を作成(ストレージの要求) 、k8s が PV を PVC に割り当てる。

- PV は事前に作成される静的なものと、PVC に応じてプロビジョニングされる動的なものがある
- 使用中のストレージオブジェクトは、ユーザーによって削除されても、すぐには削除されず、Pod で使用されなくなるまで延期される
  - 保護されているかどうかは、Finalizers のリストに kubernetes.io/pv-protection が含まれているか確認できる
- ボリュームの使用が完了したらリソースの再クレームを許可する API から PVC オブジェクトを削除できる
  - Retain は PVC を削除してもデータは保持され、Delete の場合は PV とストレージも削除される (クラウドのリソースも含め管理可能)
- PVC は途中での拡大も可能、ファイルシステムに関しても XFS,Ext3、Ext4 の場合はボリュームサイズ変更可能
- 使用中の PV は、PVC や Pod などを削除せずに拡張が可能

## 永続ボリューム

- volumeModes は Filesystem と Block 　がある
- アクセスモードで読み書きの設定を可能であり、ReadWirteOnce (単一のノードで読み書き)や ReadOnlyMany (多数のノードで読み取り専用)　などがある
- ボリュームのフェーズは、利用可能、バウンド(クレームに紐づいている)、リリース済み(クレームが削除されたが、クラスターにまだクレームされている)、失敗がある

## 永続ボリューム要求

- アクセスモードやボリュームモードなど、永続ボリュームと同様の規則になっている

# 投影ボリューム

複数の異なるボリュームを 1 つのボリュームとしてマウントできる仕組み
現在対応敷いてるボリュームソースは secret, downwardAPI, configMap, serviceAccountToken
すべてのソースは Pod と同じ NS にある必要がある
管理がシンプルになり、セキュリティ向上にもなる

# エフェメラルボリューム

Pod のライフサイクルに紐づいた一時的なボリューム

## エフェメラルボリュームのタイプ

- emptyDir: Pod の起動時には空で、キャッシュや一時データ用
- configMap, downwardAPI, secret: 投影ボリュームもエフェメラルボリューム
- CSI: エフェメラルボリューム: 外部ストレージプラグインでエメフェラルなボリュームを作れる、ログなどに使う
- 汎用エフェメラルボリューム: 通常の PVC をエフェメラルボリュームとして利用できる仕組みで、ローカル、ネットワークのストレージを使える

# VolumeAttributesClass

ボリュームの属性を動的に管理するためのリソースで、PV や PVC の作成時に、ストレージの属性を動的に適用できるようになる

# ストレージクラス

ストレージのクラスを記述する方法を提供

- ストレージクラスリソースには、PV を動的にプロビジョニングする場合に使うフィールドである provisioner, parameters, recalaimPolicy が含まれる
  - provisioner: どのストレージプロバイダーを使うか
  - prameters: ストレージの詳細設定で provisioner ごとに変わる
  - reclaimPolicy: PV が不要になったときの対応 (Delete, Retain, Recycle)

# ボリュームの動的プロビジョニング

動的プロビジョニングがなければ、クラスター管理者が事前にプロビジョニングする必要がでてくるが、これのお陰で、事前に用意せず動的にボリュームをプロビジョニングできる。
動的プロビジョニングを利用するには、1 つ以上の StorageClass を作成する必要がある

# ボリュームのスナップショット

- スナップショットは、VolumeSnapshot、VolumeSnapshotContent の API リソースで管理される
- これらは、PersistentVolumeClaim（PVC）、PersistentVolume（PV）にそれぞれ対応
- スナップショットのプロビジョニングには、事前プロビジョニングと動的プロビジョニングの 2 つがある
- VolumeSnapShotClass は StorageClass のように、VolumeSnapshot をプロビジョニングするときにストレージのクラスに関する情報を記述方法を提供
- 各 Class は driver, deletionPolicy,parameters を含む (driver は何の CSI ボリュームプラグインを利用するか)

# CSI Volume Cloning

- 既存の PVC を dataSource に指定し、その正確なコピーを作成する
- これにより、ストレージの複製が簡単にでき、データのバックアップやテスト環境の構築がスムーズになる

# ストレージ容量

- スケジューラーは、ボリュームをプロビジョニングするのに十分な容量があるかどうかを追跡している
- スケジューラーが参照している情報が最新ではない可能性もあるので、失敗する可能性はあるが、その場合はリトライされる

# ノード固有のボリューム制限

-　クラウドサービスによってノードに接続できるボリュームの数に上限がある

- KUBE_MAX_PD_VOLS で変更も可能

# ボリュームヘルスモニタリング

- CSI ドライバーはストレージシステムから異常なボリュームを検出し、それらを PVC、Pod のイベントとして報告できる

---

# 設定のベストプラクティス

## 一般的な設定の Tips

- 意味のある場合は関連オブジェクトを単一ファイルにグループ化するのが良い
- 不必要にデフォルト値を設定すると、複雑になるので、できるだけ指定しない
- オブジェクトの説明のためにアノテーションを入れましょう
- ReplicaSet や Deployment にバインドされていない Pod は使わないでください

## Service

- ある Pod がアクセスしたいサービスがある場合には、サービスを先に作成しないとコンテナの環境変数でサービスを取得できなくなる
- DNS サーバーを利用する場合には上記の順序は関係ない
- hostPort は必要な時以外に指定すると Pod がスケジュールできる場所の数を制限することになる
  - hostIP, hostPort, protocol の組み合わせはユニークである必要がある

# ConfigMap

機密性のないデータをキーと値のペアで保存するために使用される API オブジェクト
環境固有の設定をコンテナイメージから分離できるので、アプリケーションの移植が簡単になる
1MiB を超えるデータは扱えないので、それ以上のデータはボリュームのマウントや別の DB、FS を使用
data, binaryData フィールドがある

## ConfigMap と Pod

- ConfigMap を参照して、Pod の Spec を書くことができるが、これは Pod と ConfigMap が同じ NS 内に存在する必要がある

## ConfigMap を使う

ConfigMap はデータボリュームとしてマウント可能
Pod へ直接公開せずにシステムのほかの部品としても利用できる

## イミュータブルな ConfigMap

ConfigMap を広範に使用しているクラスターではデータの変更を防ぐことで下記のメリットが有る

- 停止を引き起こす予想外の変更を防ぐ
- ConfigMap をイミュータブルにマークして監視を停止することにより、kube-apiserver の負荷を削減し、クラスターの性能が上がる
  immutable フィールドを true に設定可能

# secret

Secret はパスワードやトークン、キーなどの少量の機密データを含むオブジェクト
これにより、アプリケーションコードに機密データを記載せずにすむ
Secret はそれを使用する Pod とは独立に作成できるので、Pod の作成などのワークフローの中で漏洩する可能性が低くなる
ConfigMap に似ているが、機密データを扱うかどうかが違う。

## Secret の概要

Pod が Secret を使う方法は３種類ある。

- ボリューム内のファイルとして、Pod の単一または複数のコンテナにマウント
- コンテナの環境変数を使う
- Pod を生成するために Kubelet がイメージを Pull するときに使用する

## Secret の種類

- Opaque
  - デフォルトの Secret 型
- Service account token Secrets
  - サービスアカウントを識別するトークンを格納するために利用
- Docker Config Secrets
  - イメージの Docker レジストリにアクセスするための資格情報を格納
- Basic authentication Secret
  - ベーシック認証に必要な情報を保存するために提供される (username, password)
- SSH authentication secrets
  - SSH 認証で使用されるデータを保存するために利用 (プライベートキー)
- TLS secrets
  - TLS に通常使用される証明書とそれに関連付けられたキーを保存するために利用
- Bootstrap token Secrets
  - ノードのブートストラッププロセス中に使用されるトークン用に利用

## Secret の作成

- 作成する方法は kubectl, config file, kustomize を使ってできる。

## Secret の使用

### Secret をファイルとして Pod から使用

- アプリケーションが設定ファイル.env, .pem などのファイルを読み込んで利用する場合に利用
- 一度マウントすれば、アプリケーションの通常のファイルとして扱えるので管理しやすい
- Git にコミットされるリスクや、ファイルを管理する仕組みが必要になる

### 環境変数として利用

- アプリケーションが環境変数を通じて設定を読み込む設計の場合などに利用
- 設定が簡単でアプリケーション起動時に適用可能
- env コマンドなどから漏洩するリスクがある

### イメージを Pull するときに利用

- プライベートリポジトリのコンテナイメージを取得するときに認証が必要な場合に利用 (ECR,Docker Hub など)
- セキュアにプライベートリポジトリのイメージを取得できる
- 認証情報が期限切れになると再設定が必要

## ベストプラクティス

- Secret API とやりとりするアプリケーションをデプロイするときには RBAC のようなポリシーを利用して制限をすべき
- NS 内の watch や list リクエストはかなり橋梁区なので避けるべきで、特権的なコンポーネントに限って認めるべき
- 必要な Secret に対する Get のみを適切に管理する

# liveness、Readiness および Startup Probe

- liveness Probe
  - コンテナが正常に動作しているかを確認し、異常があれば再起動
- Readiness Probe
  - コンテナがリクエストを受け付ける準備ができているかを確認し、準備ができるまではサービスへのトラフィックを受け付けない
- Startup probe
  - アプリケーションが正しく起動したかを確認する。
  - 起動が遅いアプリケーションに対し、Liveness Probe の誤作動を防ぐ。

# コンテナのリソース管理

- Pod を指定する際に、コンテナが必要とする CPU やメモリなど各リソースの量をオプションで指定可能
- Pod 内のコンテナのリソース要求を指定すると、スケジューラはこの情報を使用して、どの Node に Pod を配置するかを決定する

## 要求と制限

- リソース要求
  - Pod やコンテナが最小限必要とするリソース量。スケジューラが Pod をノードに配置する際に考慮
- リソース制限
  - Pod やコンテナが使用できる最大リソース量。リソース制限を超えるリソースの使用が発生すると、コンテナが強制終了される可能性がある

## リソースタイプ

- CPU: コンテナが消費する CPU の時間量 gita
- メモリ: コンテナが消費するメモリ量
- ストレージ: データを格納するためのストレージ

## Pod とコンテナのリソース要求と制限

- Pod のリソース要求と制限
  - Pod 全体のリソース要求と制限を設定することができるが、通常は各コンテナごとに個別に設定される
- コンテナのリソース要求と制限
  - 各コンテナに対して CPU やメモリの要求と制限を設定する。これにより、コンテナが必要とするリソースを過不足なく確保し、他のコンテナや Pod とリソースを分け合う

## リソース要求を含む Pod がどのようにスケジュールされるか

- Pod にリソース要求が設定されていると、Kubernetes スケジューラはその要求を満たすノードに Pod を配置
- スケジューラは、ノードに十分なリソースがあるかどうかを確認し、リソース要求を超えないようにして、適切なノードを選択

## リソース制限のある Pod がどのように実行されるか

- Pod がリソース制限を設定されている場合、コンテナがその制限を超えると、コンテナは OOM (Out of Memory) エラーで強制終了されるか、CPU 制限を超えた場合は CPU Throttling が発生して処理速度が遅くなる

## ローカルのエフェメラルストレージ

- エフェメラルストレージ
  - コンテナが使用する一時的なストレージであり、コンテナが終了する際にデータが削除される
- K8s では、Pod 内で emptyDir ボリュームとして提供され、コンテナが起動するたびに新しい一時的なストレージが提供される。

## 拡張リソース

- 拡張リソース: K8s では標準の CPU やメモリ以外にも、特定のハードウェアリソース（GPU など）を 拡張リソースとして指定することができる。

# kubeconfig ファイルを使用してクラスターアクセスを組織する

- kubeconfig を使用することで、クラスターに、ユーザー、名前空間、認証の仕組みに関する情報を管理できる

## Context

kubeconfig ファイルの context 要素はアクセスパラメータを使いやすい名前でグループ化するために利用する

---

# セキュリティ

- クラウドネイティブのセキュリティは 4c (Cloud, Cluster, Contaier, Code)で考える
- Code は他の 3c のセキュリティレイヤーの恩恵を受ける
- クラウドレイヤーが脆弱であると、その上のコンポーネントは安全でないかもしれない。
  - 各クラウドプロバイダーはセキュリティの推奨事項を作成している

## 各層のセキュリティ

- インフラ(クラウド)
  - コントロールプレーンへのすべてのアクセスはインターネットへ公開されず、ネットワークアクセス制御リストで制御
  - etcd へのアクセスはコントロールプレーンのみに制限
  - クラウドプロバイダーへのアクセス権限は最低限にする
- クラスター
  - クラスターコンポーネントおよび、クラスターで実行されるアプリケーションの保護が必要
  - ２つのサービスがクラスターにあり、そのうちの 1 つがリソース枯渇攻撃に対して脆弱だともう一つも影響を受ける
- コンテナ
  - イメージをビルドする手順の一部として既知の脆弱性がないかスキャン
  - 特権ユーザーを許可しない
- コード
  - TLS 通信のみ許可
  - ポートの制限など

# Pod セキュリティの標準

- Pod に対するセキュリティの設定は Security Context を使用して、Pod 単位での特権やアクセスコントロールの定義をする
- ポリシーの種別
  - 特権：制限のかかっていないポリシー
  - ベースライン、：制限は最小化されたポリシーだが、特権昇格を防止（アプリ運用者、開発者が利用）
  - 制限：厳しく制限されたポリシーで Pod を強化するための現在のベストプラクティスに沿っている（アプリ運用者、開発者、信頼度の低いユーザー）

# クラウドネイティブセキュリティと Kubernetes

CNCF ホワイトペーパーでさまざまなライフサイクルフェーズに適したセキュリティコントロールとプラクティスが定義されている。

- Develop ライフサイクルフェーズ
- Distribute ライフサイクルフェーズ
- Deploy ライフサイクルフェーズ
- Runtime ライフサイクルフェーズ
  - アクセス、コンピューティング、ストレージの３つの領域がある

# Pod のセキュリティアドミッション

- K8s クラスター内で Pod の作成や変更を制御し、セキュリティを強化する仕組み
- ポリシー違反を防止したり、セキュリティ強化のためのラベルを用意したりする
- アドミッションラベルには、下記３種類がある。
  - enforce: ポリシーに違反したら Pod は拒否される
  - audit: ポリシー違反は監査アノテーションを追加するトリガーに鳴るが、それ以外は許可
  - warn: ポリシーに違反した場合はユーザーへの警告がトリガーされるが、それ以外は許可

# サービスアカウント

- Pod やアプリケーションが API を操作するための特別なアカウント
- 人間が使うユーザーアカウントとは違い、アプリケーションやプロセスの認証、権限管理を行うためのアカウント
- Pod に関連付けられたり、k8s API にアクセスするための認証情報をもつ
- 用途
  - Pod から k8s へのアクセス
  - CICD ツールが K8s にデプロイを行う
  - RBAC と組み合わせてアクセス制御

# Kubernetes API へのアクセスコントロール

- K8s API は 人間のユーザーとサービスアカウントの認証が可能
- RBAC を利用し、最小権限の原則を適用
- 一般的には k8s API は 443 番ポートで通信
- TLS が確立されると、認証のステップに入る(クライアント認証や jwt などを利用)
- 認証のあとには認可を実施
- アドミッションコントロールにより、リクエストを変更、拒否などの制御が可能

# ロールベースアクセスコントロールのグッドプラクティス

- 最小権限の原則: ユーザーや Pod に必要最低限の権限のみを付与
- 特権トークンの配布を最小限に抑える
- デフォルトで提供される RBAC 権限を適切にする
- 特権昇格リスク（ Secret の List を適切に管理、ワークロードの作成、永続ボリュームの作成などを適切に管理）

# Kubernetes Secret の適切な使用方法

- クラスター管理者
  - データを etcd で保管するときに暗号化をする
  - Secret への最小特権アクセスを設定
  - etcd の管理ポリシーを改善 (使用しなくなった場合は削除かシュレッダーで処理)
- 開発者
  - 特定のコンテナへの secret アクセスを制限
  - 読み取り後に Secret データを保護
  - Secret マニフェストの共有を避ける (base64 でデータをエンコードしている場合は読まれてしまう)

# Multi-tenancy

- K8s でマルチテナント環境を実現する際の考慮点について
- 名前空間の分離: 各テナントごとに Namespace を用意し、リソースを分離
- Network Policies: テナント間の通信を制御し、不正アクセスを防止
- ResourceQuota: リソースの使用制限を設定し、他のテナントへの影響を最小化
- Pod Security Standards: 各テナントの Pod に対してセキュリティポリシーを適用

# 堅牢化ガイド - 認証メカニズム

- 認証を強化し、クラスタへの不正アクセスを防ぐもの。ユーザー管理単純化のため使用する認証方法はできるだけ少なくすることが推奨
- 方法
  - X509 クライアント証明書
  - 静的なトークンファイル
  - ブートストラップトークン
  - サービスアカウントシークレットトークン
  - TokenRequestAPI トークン
  - OIDC
  - Webhook トークン
  - 認証プロキシ

# Linux kernel security constraints for Pods and containers

- Pod やコンテナの Linux セキュリティを強化するための対策
- Seccomp プロファイル: 不要なシステムコールをブロック
- AppArmor/SELinux: プロセスのアクセス権限を細かく制限、これにより、Pod やコンテナがアクセスできるリソースを制限し、セキュリティを強化できる

---

# Limit Range

- デフォルトではコンテナはリソース制限がないが、リソースクォータを使うことで、クラスター管理者はリソースの消費と作成を制限可能
- 管理者は 1 つの NS に 1 つの LimitRange を作成し、その Limit を越えようとすると API サーバーへのリクエストが 403 になる

# リソースクォータ

- cpu, memory に加え、ストレージ、オブジェクト数 (サービスやシークレットの数) のクォータもある
- クォータのスコープで、クォータを適用する範囲を制限できる
  - Terminating, BestEffort, PriorityClass など
- 最低使用量と最大使用量は request と limit で記載
- リソースクォータは集約されたクラスターリソースを分割するが、ノードに対しては制限をしないので、特定のノードへの負荷が集中する可能性がある

# プロセス ID の制限と予約

- Pod やノードごとに割りあて可能なプロセス ID の数を制限することができる
- これがないと、Pod やノードで利用可能なプロセスがすべて消費され、ワークロードが停止などのリスクが有る

# ノードリソースマネージャ

- レイテンシが重要だったり、高スループットのワークロードをサポートするために、リソースマネージャがある。
- これは、CPU、メモリなどの要件が設定された Pod のためにノードリソースの最適化をする (リソースクォータではノードの管理まではないので)
