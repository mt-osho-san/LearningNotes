# 概要

## Kubernets のコンポーネント

- k8s はコントロールプレーンのコンポーネントとノードというマシン郡で構成されている。

### コントロールプレーンコンポーネント

- コントロールプレーンコンポーネントはシンプルにするために、すべてのコントロールプレーンコンポーネントを同じマシンで起動し、そのマシンではユーザーコンテナは実行しない
- kube-apiserver
  - k8s api を外部に提供する k8s のコントロールプレーンのコンポーネントで、コントロールプレーンのフロントエンドになる
- etcd
  - 一貫性、高可用性を持ったキーバリューストアで、k8s のすべてのクラスター情報の保存場所として利用される
- kube-scheduler
  - コントロールプレーン上で動作するコンポーネントで、新しく作られた Pod にノードが割り当てられているか監視し、割り当てられていなかった場合にその Pod を実行するノードを選択
  - Pod のリソース要求量やハード、ソフトの制約、アフィニティ、有効期限などを考慮して行われる
- kube-controller-manager
  - 下記の複数プロセスを実行
    - ノードコントローラー：ノードがダウンした場合の通知と対応を担当
    - Job コントローラー：単発タスクを表す Job オブジェクトを監視し、そのタスクを実行して完了させるための Pod を作成 (単発タスクはどういうものがあるのか)
    - EndpointSlice コントローラ：EndpointSlice オブジェクトを作成 (Service と Pod を紐づける)
    - ServiceAccount コントローラー：新規の名前空間に対してデフォルトの ServiceAccount を作成する
- cloud-controller-manager
  - クラウド上で実行する際に利用するもので、クラウドプロバイダー固有のコントローラーのみを実行する

### ノードコンポーネント

ノードコンポーネントはすべてのノードで実行され、稼働中の Pod 管理や k8s の実行環境を提供する。

- kubelet
  - クラスター内の各ノードで実行されるエージェントで、各コンテナが Pod で実行されていることを保証する
  - PodSpec のセットを取得し、それらの PodSpec に記述されているコンテナが正常に実行されている状態を保証する
- kube-proxy
  - kube-proxy はクラスター内の各 node で動作しているネットワークプロキシ
- コンテナランタイム
  - コンテナの実行を担当するソフトウェアで、Docker、containerd, CRI-O, すべての k8s CRI をサポート

### アドオン

アドオンはクラスターレベルの機能を提供しているので、アドオンのリソースで名前空間が必要なものは kube-system 名前空間に属する

- DNS
  - クラスター DNS 以外のアドオンは必須ではないが、すべての k8s クラスターはクラスター DNS を持つべき
  - クラスター DNS は環境内の他の DNS サーバーに加えて、k8s サービスの DNS レコードを提供する DNS サーバー
- WebUI
  - クラスター内で実行されているアプリケーションについて管理、トラブルシューティングができる Web ベース UI
- コンテナリソースの監視
- クラスターレベルのロギング

---

## Kubernetes API

k8s 内のオブジェクトの状態をクエリで操作できる。
k8s のコントロールプレーンの中核は API サーバーとそれが公開する HTTP API
ユーザー、クラスターのさまざまな部分、および外部コンポーネントはすべて API サーバーを介して互いに通信する。

### 永続性

- k8s は API リソースの観点からシリアル化された状態を etcd に書き込むことで保存する

### API の変更

k8s は既存のクライアントとの互換性を破壊しない、互換性を一定期間維持して、他のプロジェクトが適応する機会を提供する

---

# k8s オブジェクトを理解する

k8s は永続的なエンティティで、下記を表現可能

- どのようなコンテナ化されたアプリケーションが稼働しているか
- それらのアプリケーションが利用可能の名リソース
- アプリケーションがどのように振る舞うかのポリシー、再起動、アップグレードなど
  k8s は意図の記録で、一度オブジェクトを作成すると、k8s は常にそのオブジェクトが存在し続けるように動く

### オブジェクトの spec(仕様)と status(状態)

ほとんどの k8s オブジェクトはオブジェクトの設定を管理する 2 つの入れ子になったオブジェクトのフィールドを持っている (spec, status)
spec を持っているオブジェクトは作成時に spec を設定する必要があり、望ましい状態としてオブジェクトに持たせたい特徴を記述する必要がある
status オブジェクトは、オブジェクトの現在の状態を示し、k8s のコントロールプレーンは望ましい状態と status が一致するように管理をする

- 必須フィールド
  - apiVersion
  - kind: どの種類のオブジェクトを作成するのか
  - metadata: オブジェクトを一意に特定するための情報
  - spec: 望ましい状態

---

# Kubernetes オブジェクト管理

### 管理手法

- 命令型コマンド
  - 現行のオブジェクトを対象にし、開発用プロジェクトで利用される
- 命令型オブジェクト設定
  - 個々のファイルを対象にし、本番用で使われる
- 宣言型オブジェクト設定
  - ファイルのディレクトリを対象にし、本番用で使われる

---

# オブジェクトの名前と ID

クラスター内のオブジェクトにはそのタイプのリソースに固有の名前があり、クラスター全体で一位の UID もある。
同じ名前空間内に myapp-1234 という名前の Pod は 1 つしか含められないが、myappp-1234 という名前の 1 つの Pod と 1 つの Deployment を含めることができる。
一意でない属性の付与のためにはラベルとアノテーションがある

## 名前

クライアントから提供され、オブジェクトを参照する文字列 (ex. /api/v1/pods/<名前>)
同じ種類のオブジェクトでは同じ名前を同時には持てないが、削除すればできる
以下３つはよく使われる命名規則

- DNS サブドメイン名
- DNS ラベル名 => ドメイン名の . で区切られた部分のこと
- パスセグメント名

## UID

オブジェクトを一意に識別するための k8s が生成する文字列 k8s クラスターの生存期間中にわたって生成されたすべてのオブジェクトは異なる UID を持っている

---

# ラベルとセレクター

ラベルはオブジェクトに割り当てられるキーとバリューのペア
ラベルはオブジェクトをグルーピングするために使うことができ、いつでも追加修正が可能

## ラベルセレクター

ラベルセレクターを介して、クライアントとユーザーはオブジェクトのセットを指定可能。
k8s API がサポートするセレクターは等価ベースと集合ベース

- 等価ベース
  - =,==,!=
- 集合ベース
  - in, notin, exists

---

# 名前空間

k8s は同一の物理クラスター上で複数の仮想クラスターの動作をサポートするが、この仮想クラスターを NameSpace と呼ぶ

## 複数の NS を使うとき

複数のチーム、プロジェクトにまたがる多くのユーザーがいる環境での試用を目的としている。
数人から数十人のクラスターに対して NS を作成する必要はない
同じアプリケーションの異なるバージョンなど、少し違うリソースをただ分割するだけに複数の NS を使う必要はない。
=> この場合は NS 内でリソースを区別するためにラベルを使う

## NS を利用する

`kube get namespace` によってクラスター内の NS の一覧を表示できる
k8s の起動時には 4 つの初期 NS が作成されている

- default: 他に NS を持っていないオブジェクトのためのデフォルト NS
- kube-system: k8s システムによって作成されたオブジェクトのための NS
- kube-public: すべてのユーザーから読み取り可能
- kube-node-lease: クラスターのスケールに応じたノードハートビートのパフォーマンスを向上させる各ノードに関連した Lease オブジェクトのための NS
- NS の設定
  `kubectl run nginx --image=nginx --namespace=<insert-namespace-name-here>`

## すべてのオブジェクトが NS に属しているとは限らない

Node や PersistentVolume のような低レベルのリソースはどの NS にも属さない

```
# NS に属しているものを確認するコマンド
kubectl api-resources --namespaced=true
```

---

# アノテーション

ユーザーは識別用途でない任意のメタデータをオブジェクトに割り当てるためにアノテーションを使用できる
アノテーションはラベルと同様にキーバリューのマップとなる
下記はアノテーションの利用例

- ビルド、リリースやタイムスタンプのようなイメージの情報
- ロギング、管理分析用のポインタ
- デバッグ目的で使用されるためのクライアントライブラリやツールの情報

---

# フィールドセレクター

１つ以上のリソースフィールドの値を元に k8s リソースを選択するためのもの。
status.phase が running の Pod をすべて選択している例: kubectl get pods --field-selector status.phase=Running

---

# ファイナライザー

削除対象としてマークされたリソースを削除する前に特定の条件が満たされる魔 dk8s を待機させるための名前空間付きキー
Pod 削除時に PersistentVolume を開放する必要があり、ファイナライザーが PV の解放を担当し、解放が完了するまで Pod は削除されない
=> 整合性を担保している

## どのようにファイナライザーは動作するか

マニフェストファイルを使ってリソースを作るときに metadata.finalizers フィールドで指定できる

---

# オーナーと従属

いくつかのオブジェクトは他のオブジェクトのオーナーになっている。
例えば ReplicaSet は Pod の集合のオーナー

## オーナーリファレンス

従属オブジェクトはオーナーオブジェクトを参照するためのフィールドを持っている。
従属オブジェクトはオーナーオブジェクトが削除されたときにガベージコレクションをブロックするかどうかを管理する ownerReference.blockOwnerDeletion フィールドも持っている

---

# 推奨ラベル

共通のラベルを使うことで、様々なツールに対応できるようになるというもの
例としては、app.kubernetes.io/name や app.kubernetes.io/instance などがある。

---

# ノード

ノード上で実行される Pod にコンテナを配置してワークロードの実行をする。
ノードは 1 つの VM または物理的なマシン
1 つのノード上のコンポーネントには kubelet, コンテナランタイム、kube-proxy が含まれる (すべてのノードに？)

## 管理

ノードを API サーバーに加えるには２つの方法がある (クラスターに加えるということ？)

1. ノード上の kubelet がコントロールプレーンに自己登録する

- kubelet のフラグ --register-node が true のとき、自身を登録しようとする。これは推奨されているパターン

2. ユーザーが手動で Node オブジェクトを追加

- 上記フラグを false にする。

## ノードのステータス

- Address
  - HostName,ExternalIP,InternalIP などがある
- Conditions
  - Ready: ノードの状態が有効で Pod を配置可能な場合に True になる
  - DiskPressure: ディスク容量が圧迫されているときに True になる
  - PIDPressure: プロセスが圧迫されているときに True になる
- Capacity と Allocatable
- Info: カーネルや k8s,Docker のバージョンなど

## ノードのハートビート

ハートビートは k8s ノードから送信され、ノードが利用可能化判断するのに役立つ

1. Node の .status 更新
2. Lease object?

## ノードコントローラー

ノードの存続期間中に下記役割を果たす

1. ノードが登録されたときに CIDR ブロックをノードに割り当てる
2. ノードコントローラーの内部ノードリストをクラウドの利用可能なマシンと一致させる
3. ノードの状態を監視

# ノードとコントロールプレーン間の通信

信頼できないネットワーク上でクラスターが実行できるようにしている。

## ノードからコントロールプレーンへの通信

ノードからのすべての API は API サーバーで終了し、コントロールプレーンコンポーネントはどれもリモートサービスを公開するようにはなっていない。
API サーバーは 1 つ以上の形式のクライアント認証が有効になっている状態で、セキュアな HTTPS ポートでリモート接続をリッスンするように設定されている
ノードは、有効なクライアント認証情報とともに、API サーバーに安全に接続できるように、クラスターのパブリックルート証明書でプロビジョニングされる必要がある
これらにより、ノード上で動作する Pod からコントロールプレーンへの接続はデフォルトでセキュアで、信頼されていないネットワークやパブリックネットワークを介して実行可能

## コントロールプレーンからノードへの通信

2 つの経路がある

1. API サーバーからクラスター内の各ノードで実行される kubelet プロセスへの通信経路

- Pod のログの取得、実行中の Pod へのアタッチ、kubelet のポート転送機能の提供に使われる
- kubelet の HTTPS エンドポイントで終了する接続だが、デフォルトでは kubelet サービング証明書を検証しないので、信頼できないネットワークを解するのは安全じゃない
- よって、サービング証明書を検証するか、SSH トンネルを使うなどする (SSH トンネルは現在は非推奨で、Konnectivity を使うべし)

2. API サーバーのプロキシ機能を介した API サーバーから任意のノード、Pod、サービスへの通信経路

- デフォルトで HTTP なので、認証も暗号化もされない。
  => 何に使う？

## konnectivity サービス

SSH トンネルの代替として、コントロールプレーンからクラスターへの通信に TCP レベルのプロキシを提供

# リース

共有リソースをロックしたり、ノード間の活動を調整する機構
k8s においては、coordination.k8s.io API グループの Lease オブジェクトに表されており、ハートビートやコンポーネントレベルのリーダー選出といったシステムに利用されている

## ハートビート

各ノードに Lease オブジェクトがあり、それに対して Update リクエストを送り、spec.renewTime フィールドを更新する
コントロールプレーンはこのフィールドのタイムスタンプを見て、Node が利用可能か判断する

## リーダー選出

あるコンポーネントのインスタンスが常にひとつだけ実行されていることを保証するのにも利用される

## API サーバーのアイデンティティー

kube-apiserver は LeaseAPI を利用して、自身のアイデンティティーをその他のシステムに向けて、公開するようになった
これは kube-apiserver が何台のコントロールプレーンを稼働させているのかをクライアントが知るためのメカニズムを提供する
kube-apiserver 間の調整が必要な場合に使える

# コントローラー

クラスターの状態を監視し、必要に応じて変更を加えたり、要求したりする制御ループ

## コントローラーパターン

k8s のオブジェクトには目的の状態を表す spec フィールドがあり、この値に近づける責務をコントローラーが持つ

## API サーバー経由でコントロール

Job コントローラーが新しいタスクを見つけると、その処理が完了するように、kubelet が正しい数の Pod を実行することを保証する
しかし、Job コントローラーは自分自身で Pod やコンテナを実行することはなく、API サーバーに依頼をする
Job では目的の状態となると完了し、Finished マークが付く

## 直接的なコントロール

Job とは対象的にクラスターの外部に変更を加える必要があるコントローラーもある
例えば、クラスターに十分な数の Node が存在することを保証する制御ループの場合、そのコントローラーは必要に応じて新しい Node をセットアップするために、外部とやりとりをする
コントローラーが目的の状況を実現するために変更を加えてから、現在の状態をクラスターの API サーバーに報告する
エアコンの例では、非常に寒いときは別のコントローラーが霜防止ヒーターをオンにするようなケース

# クラウドコントローラーマネージャー

クラウド特有の制御ロジックを組みこむコントロールプレーンのコンポーネント
コンポーネント間の密なつながりが不要な設計なので、ハイブリッドでも利用可能

## クラウドコントローラーマネージャーの機能

### ノードコントローラー

クラウドインフラで新たなサーバーが作成されたときに、Node オブジェクトを作成する責務を持つ

### ルートコントローラー

クラスター内の異なるノード上で稼働しているコンテナが相互に通信できるようにクラウド内のルートを適切に設定する責務を持つ

### サービスコントローラー

マネージドロードバランサー、IP アドレスネットワークパケットフィルタやヘルスチェックのようなクラウドインフラコンポーネントのインテグレーションを行う

# cgroup v2 について

Linux ではコントロールグループがプロセスに割り当てられるリソースを制限している
kubelet と基盤となるコンテナランタイムは cgroup をインターフェースとして接続する必要がある

## cgroup v2 とは

v1 からの改善点

- 単一階層設計の API
- より安全なコンテナへのサブツリーの移譲
- 強化されたリソース割り当て管理と複数リソース間の隔離

## cgroup v2 を使う

おすすめの方法はデフォルトで v2 が有効で使える Linux ディストリビューションを使うこと

# コンテナランタイムインターフェース

クラスターコンポーネントを再コンパイルせずに kubelet がさまざまなコンテナランタイムを使用できるようにするプラグインインターフェース

## API

kubelet は gRPC を介してコンテナランタイムに接続するときにクライアントとして機能する
コンテナランタイムは --image-service-endpoint コマンドラインフラグで kubelet 内で個別に設定可能

# ガベージコレクション

k8s がリソースをクリーンアップするために使用するさまざまなメカニズムの総称

## カスケード削除

k8s は owner reference がなくなったオブジェクトをチェックして削除する
カスケード削除ではオブジェクトの依存関係を自動的に削除するかを制御可能であり、下記 2 つの種類がある

### フォアグラウンドカスケード削除

- 削除するオーナーオブジェクトは最初に削除進行中になる
- API サーバーはオブジェクトの metadata.deletionTimeStamp フィールドをオブジェクトに削除のマークが付けられた時刻に設定する
- API サーバーは metadata.finalizers フィールドを foregroundDeletion に設定
- オブジェクトは削除プロセスが完了するまで、k8s API を介して表示されたままになる

### バックグラウンドカスケード削除

- API サーバーがオーナーオブジェクトをすぐに削除し、コントローラーがバックグラウンドで依存オブジェクトをクリーンアップする
- これはデフォルト

## 未使用のコンテナとイメージのガベージコレクション

kubelet は未使用のイメージに対して 5 分ごとに未使用のコンテナに対して 1 分ごとにガベージコレクションを実行する
ガベージコレクションの時間などは設定ファイルで調整可能

# Mixed version proxy

複数のバージョンを併用して利用する場合に、バージョンの歳による予期しない 404 エラーなどを防ぐ

---

# コンテナ

アプリケーションとランタイムの依存関係を一緒にパッケージ化できる

# イメージ

コンテナイメージはアプリケーションと依存関係のあるソフトウェアをすべてカプセル化したバイナリデータ

## イメージの名称

レジストリのホスト名やポート番号を含むこともある (レジストリのホスト名を指定しない場合は Docker パブリックレジストリを意味しているとみなす)
タグを追記して、バージョンを識別することもある

## イメージの更新

デフォルトでは Pod 内のコンテナの Pull ポリシーは明示的に指定されていない場合、IfNotPresent に設定される
イメージがすでに存在する場合、このポリシーは Kubelet にイメージの Pull をスキップさせる

### イメージ Pull ポリシー

kubelet が指定されたイメージを Pull しようとするときに影響するもの

- IfNotPresent
  - イメージがローカルにない場合のみイメージが Pull される
- Always
  - コンテナを起動するときは毎回レジストリに照会する
  - ローカルにキャッシュされ r た同一ダイジェストのイメージがあったらそれを利用
- Never

  - イメージを取得しようとせずに、ローカルにイメージがある場合のみコンテナを起動する

- Pod がいつも同じバージョンのコンテナイメージを使用するために、イメージのダイジェストを指定することもできる => <image-name>:<tag> ではなく <image-name>@<digest> を利用
- 同一のイメージタグで、そのコードを変更すると、新旧のコードが混在する可能性があるが、ダイジェストは一意なので、混在を避けるときには有効

## イメージインデックスを使ったマルチアーキテクチャイメージ

コンテナのアーキテクチャ固有バージョンに関する複数のイメージマニフェストを指すことができる

## プライベートレジストリ

認証の方法

- プライベートレジストリへの認証を Node に設定する
- 事前に Pull されたイメージ
- Pod で ImagePullSecrets を指定する
- ベンダー固有または、ローカルエクステンション

ユースケースによって、プライベートレジストリの認証方法やパブリックレジストリとの併用などもある

# コンテナ環境

k8s はコンテナにいくつかのリソースを提供(イメージとファイルシステム、コンテナ自体の情報、クラスター内のオブジェクトの情報)

## コンテナ情報

コンテナのホスト名はコンテナが実行されている Pod の名前で hostname で呼び出し可能

## クラスター情報

コンテナの作成時に実行されていたすべてのサービスのリストは環境変数として使用できる

# ランタイムクラス

RuntimeClass はコンテナラインタイムの設定を選択する機能で、コンテナラインタイム設定は Pod のコンテナを稼働させるために使われる

## RuntimeClass を使う動機

- 異なる Pod に異なる RuntimeClass を設定することで、パフォーマンスとセキュリティのバランスをとることができる。
- ワークロードの一部に高レベルの情報セキュリティ保証が必要な場合、ハードウェア仮想化を使用するコンテナランタイムで実行されるようにそれらの Pod をスケジュールすることを選択できる
- RuntimeClass を使って、コンテナランタイムは同じで設定が異なる Pod を実行することもできる。

## セットアップ

手順

- ノード上で CRI 実装を設定
- 対応する RuntimeClass を作成

## スケジューリング

scheduling フィールドを指定することで、設定された RuntimeClass をサポートするノードに Pod がスケジューリングされるように制限ができる

# コンテナライフサイクルフック

フックにより、コンテナは管理ライフサイクル内のイベントを認識し、対応するライフサイクルフックが実行されたときにハンドラーに実装されたコードを実行できる

## コンテナフック

コンテナに公開されている 2 つのフックがある

- PostStart
  - コンテナが作成された直後に実行される
- PreStop
  - API からの要求、liveness/startup probe の失敗、プリエンプションなどの管理イベントが原因でコンテナが終了する直前に呼び出される

---

# ワークロード

ワークロードとは、k8s 上で実行中のアプリケーションであり、下記の種類がある

- Deployment,ReplicaSet
- StatefulSet
- DaemonSet
- Job と CronJob

# Pod

Pod は k8s 内で作成、管理できる最小のデプロイ可能なユニットで、1 つまたは複数のコンテナのグループであり、ストレージやネットワークなどの共有リソースを持つ

## Pod とはなにか

Pod の共有コンテキストは、namespaces,cgroups などの隔離技術を用いて作られる

## Pod を使用する

下記のようなコマンドで実行できる

```
kubectl apply -f https://k8s.io/examples/pods/simple-pod.yaml
```

### Pod を管理するためのワークロードリソース

単一のコンテナしかもたないシングルトンの Pod でも直接作成するのではなく、Deployment や Job などのワークロードリソースを使用して作成する
Pod が状態を保持する必要がある場合は StatefulSet リソースを使用する
k8s の Pod は主に次の２つの方法で使われる

- 単一のコンテナを稼働させる Pod: 一般的なユースケース
- 強調して稼働させる必要がある複数のコンテナを稼働させる Pod

### Pod が複数のコンテナを管理する方法

単一の Pod 内の複数のコンテナはクラスター内の同じ物理または仮想マシン上で自動的に同じ場所に配置される
コンテナ間ではリソースや依存関係を共有したりできる

## Pod を利用する

Pod を直接つくることがないのは、Pod は一時的で、使い捨てできる存在として設計されたものであるから。

### Pod OS

.spec.os.name で windows か linux かを設定できる
ノードの OS がこれと異なる場合には Pod の実行は拒否される。

### Pod とコンテナコントローラー

ワークロードリソースは複数の Pod を作成、管理することができ、複製やロールアウト、障害時の自動回復を行う

## Pod の更新と取り替え

ワークロードリソースの Pod テンプレートが変更されると、既存の Pod を更新したり、パッチ適用するのではなく、更新されたテンプレートに基づいて新しい Pod を作成する
しかし、patch, replace などのアップデート操作には一部制限がある

- namespace, uid, creationTimeStamp は変更できない
- metadata.deletionTimestamp がある場合、metadata.finalizers に新しい項目を追加することはできない
  など

## リソースの共有と通信

Pod はデータの共有と構成するコンテナ間での通信を可能にする。

### Pod 内のストレージ

Pod 内のすべてのコンテナは共有ボリュームにアクセスできるので、複数のコンテナでデータを共有できる
Pod 内の１つのコンテナで再起動するときも、Pod 内の永続化データを保持できる

### Pod ネットワーク

各 Pod にはユニークな IP アドレスが割り当て割れる
Pod の中では localhost を使用して他のコンテナと通信できる

## コンテナの特権モード

Pod 内のどのコンテナも privileged フラグを設定することで、特権モードを有効にできる
OS の管理者権限が必要な場合に使える

## static Pod

API サーバーには管理されない、特定のノード上で kubelet デーモンに直接管理される Pod のこと
static pod は常に特定のノード上の 1 つの kubelet に紐づけられ、主な用途は、kubelet を使用して、個別のコントロールプレーンコンポーネントを管理すること

## コンテナの Probe

kubelet がコンテナに対して行うヘルスチェック

# Pod のライフサイクル

Pending => Running => Suuceeded or Failed
Pod は生存期間に一回だけスケジューリングされ、Pod は停止 or 終了までその Node 上で実行される

## Pod のライフタイム

アプリケーションコンテナと同様に Pod は比較的短期間の存在。
ノードが停止した場合、その Node にスケジュールされた Pod はタイムアウト時間の経過後に削除される
特定の Pod は新しいノードに再スケジュールされずに、同じ名前で新しい UID をもつ同一の Pod に置き換える

## Pod のフェーズ

取りうる値

- Pending, Running,Succeeded, Failed, Unknown

## コンテナのステータス

Pod のフェーズと同様に k8s は pod 内の各コンテナの状態を追跡する。取りうる値は下記。

- waiting
- Running
- Terminated

## コンテナの再起動ポリシー

Pod の spec には、Always,OnFailure,Never のいずれかの値を持つ restartPolicy フィールドがあり、デフォルトは Always

## Pod の Condition

Pod には、PodStatus があり、Pod が成功したかどうかなどの下記 PodCondition が含まれている
type, status, lastProbeTime, lastTransitionTime, reason, message

## コンテナの Probe

probe を行うためには、kubelet はコンテナ内でコードを実行するか、ネットワークリクエストする。

### チェックのメカニズム

4 つの異なる方法があり、各 probe はどれか 1 つを定義する必要がある

- exec
  - コンテナ内で特定のコマンドを実行
- grpc
- httpGet
- tcpSocket
  - ポートが空いていれば OK

### Probe の結果

Success, Failure, Unknown

### Probe の種類

- liveness Probe
  - コンテナが動いているかを示す
- readiness Probe
  - コンテナがリクエスト応答する準備ができているかを示す
- startupProbe
  - コンテナ内のアプリケーションが起動したかどうかを示す

## Pod の終了

Pod は不要になったときに、正常に終了できることが重要

### Pod の強制削除

デフォルトではすべての削除は 30s 以内に正常に行われるが、独自の値を指定可能で、0 にすると、強制削除になる

### 終了した Pod のガベージコレクション

失敗した Pod は人間またはコントローラーが明示的に削除するまで残る
コントロールプレーンは終了状態の Pod の数が閾値を超えたときに、それらの Pod を削除する

# Init コンテナ

アプリケーションコンテナの前に実行される特別なコンテナ
アプリケーションコンテナのイメージに存在しないセットアップスクリプトやユーティリティーを含めることができる

## Init コンテナを理解する

Init コンテナは下記を除いて通常のコンテナと全く同じ

- Init コンテナは常に完了するまで稼働
- 各 Init コンテナは次の Init コンテナが稼働する前に正常に完了する必要がある

Init コンテナが失敗すると何度も再起動するが、restartPolicy が Never ならば、その Pod 全体が失敗したということになる

### 通常のコンテナとの違い

Pod の準備ができる前に完了するので、lifecycle,livenessProbe,readinessProbe, startupProbe はサポートしない

## Init コンテナを使用する

ユースケース

- シェルコマンドを使ってサービスが作成されるのを待機

```
for i in {1..100}; do sleep 1; if nslookup myservice; then exit 0; fi; done; exit 1
```

- コンテナ起動を待機 sleep 60
- git リポジトリを Volume にクローン

# サイドカーコンテナ

メインのアプリケーションコンテナと同じ Pod で実行されるセカンダリーコンテナ
主要なアプリケーションコードを直接変更することなく、ロギング、モニタリング、セキュリティなどの機能を提供する

## サイドカーコンテナの有効化

SidecarContainers というフィーチャーゲートにより、指定可能
同じポッド内の他の init コンテナやアプリケーションコンテナとは独立しており、再起動などは独立して行える

# Disruption

## 自発的な Disruption と非自発的な Disruption

非自発的な Disruption

- ハードウェア障害
- 管理者が誤って削除
- カーネルパニック
- ノードのリソース不足による Pod の退避

自発的な Disruption

- Deployment やその他の Pod を管理するコントローラーの削除
- 再起動を伴う Deployment 内の Pod のテンプレートの更新
- Pod の直接削除

## Disruption への対応

非自発的な Disruption に対応する方法の例

- Pod が必要なリソースを要求するようにする
- 高可用が必要な場合はアプリケーションをレプリケートする
- レプリケートされたアプリケーションを実行する際にさらに高い可用性を得るにはラックを横断などでアプリケーションを分散させる

## Pod Disruption Budget

自発的な Disruption が頻発する場合でも可用性の高いアプリケーションの運用を支援する機能を提供している
各アプリケーションに対して PDB を作成できる。
これは、レプリカを持っているアプリケーションのうち、自発的な Disruption によって同時にダウンする Pod の数を制限する
例えば kubectl drain で指定した最低数よりも Pod が少なくなる場合にはドレインはブロックされる。

## クラスターオーナーとアプリケーションオーナーロールの分離

クラスターとアプリケーションのオーナーは別のロールであると考えると良い

- 多くのアプリケーションチームで k8s クラスターを共有している
- クラスター管理を自動化するためにサードパーティのツールやサービスを使用している場合

## クラスターで破壊的なアクションを実行する方法

ノードやシステムソフトウェアのアップグレードなどで、破壊的なアクションを実行する必要がある場合、次のような選択肢がある

- アップグレードの間のダウンタイムを許容
- もう一つの完全なレプリカクラスターにフェールオーバーする
- Disruption に体制のあるアプリケーションを作成し、PDB を使用

# エフェメラルコンテナ

すでに存在する Pod 内で一時的に実行するコンテナ
アプリケーションの構築ではなく、サービスの調査のために利用する

## エフェメラルコンテナを理解する

トラブルシューティングのためにすでに存在する Pod の状態を調査する必要が出てくることがある
このとき、既存の Pod 内でエフェメラルコンテナを実行することで、Pod の状態を調査したり、コマンドを実行したりする

### エフェメラルコンテナとはなにか

他のコンテナとは異なり、リソースや実行が保証されず、自動的に再起動されることもないので、アプリケーション構築には適さない

# Pod Quality of Service Classes

ノード上のリソースが不足した際に、どの Pod を優先的に維持し、どの Pod を停止するかを決定するのに使われる

## Quality of Service classes

使用可能なクラスは、Guaranteed,Burstalbe,BestEffort がある。

## Guaranteed

最も厳しいリソース制限があり、排除される可能性が最も低い

### Criteria

ポッド内のすべてのコンテナにはメモリ制限とメモリ要求が必要
ポッドのすべｔねおコンテナについて、メモリ制限はメモリ要求と等しい必要がある
ポッド内のすべてのコンテナには CPU 制限と CPU 要求が必要
ポッド内のすべてのコンテナの CPU 制限は CPU 要求と等しくなければならない

## Burstable

下限リソース保証があるが、特定の制限を必要としない Pod

### Criteria

Pod は Garantedd の基準を満たしていない
ポッド内の少なくとも１つのコンテナにメモリか CPU 要求か制限がある

## BestEffort

他の QoS クラスの Pod に割り当てられていないノードリソースを使用できる

### Criteria

上記 2 つの QoS クラスを満たさない場合

# ユーザー名前空間

ホストのユーザーとコンテナ内プロセスが利用するユーザーを隔離するもの
これにより、コンテナ内で root として稼働するプロセスをホスト側の異なるユーザーとして実行できる
ホストや他の Pod に危害を及ぼす、侵害されたコンテナによる被害を軽減することができる

# Downward API

K8s が Pod やコンテナに関する情報を内部で動作しているアプリケーションに提供する仕組み
Pod やコンテナに関するメタデータやリソース情報をアプリケーションが動的に取得できる

---

# Deployment

Deployment で理想状態を記述すると、Deployment コントローラーは指定された頻度で状態を変更する

## ユースケース例

- ReplicaSet をロールアウト
- より多くの負荷をさばけるように Deployment をスケールアップ

## 古いリビジョンのクリーンアップポリシー

Deployment が管理する古い ReplicaSet を自動で削除する .spec.revisionHistoryLimit というフィールドがある。

## Deployment Spec の記述

- Pod テンプレート
  - .spec.template は Pod テンプレートで、apiVersion や kind がないこと以外は Pod と同じスキーマ
- セレクター
  - .spec.selector は必須フィールドで、Deployment によって対象とされる Pod のラベルセレクターを指定

# ReplicaSet

ReplicaSet の目的は、どのようなときでも安定したレプリカ Pod のセットを維持することであり、レプリカ数の Pod が指定した数利用可能であることを保証するものとして使われる。

## ReplicaSet がどのように動くか

ReplicaSet が対象とする Pod をどう特定するかを示すセレクターや稼働させたい Pod のレプリカ数、テンプレートなどとともに定義される
Pod と連携は、metadata.ownerReferences というフィールドを使って行われる

## ReplicaSet をつかうとき

Deployment はより上位の概念で、Pod のアップデート機能などの有益な機能があるので、ReplicaSet を直接使うよりも、Deployment の利用が推奨
=> 使うことはほとんどない？
シンプルな機能のみで良い場合は ReplicaSet のほうが管理コストが少ない

# StatefulSet

ステートフルなアプリケーションを管理するための API
Pod のデプロイとスケーリングを管理し、Pod の順序と一意性を保証する

## StatefulSet のユースケース

- データの永続化が必要な場合
- 特定の起動順序が必要な場合
- 一意のネットワーク ID が必要な場合
- 分散アプリケーションの構築

## Pod アイデンティティー

StatefulSet の Pod は順序を示す番号、安定したネットワークアイデンティティー、安定したストレージからなる一意なアイデンティティーを持つ。

## デプロイとスケーリングの保証

- Pod がデプロイされるとき、それらの Pod は 0,1,2... の番号で順番に作成される
- Pod が削除される時、降順に削除される
- Pod にスケーリングオプションが適用される前に、その Pod の前の順番のすべての Pod が Runnnig かつ Ready になっている必要がある
- Pod が停止される前に、その Pod の番号よりも大きい番号のすべての Pod はシャットダウンしている必要がある

# DaemonSet

ノード上に Pod が 1 つ確実に実行していることを保証するためのリソース
ノードがクラスターに追加される時、Pod が Node 上に追加され、Node がクラスターから削除されたとき、GC により除去される
ユースケースは、Node

## Daemon Pod がどのようにスケジューリングされるか

DaemonSet コントローラーは対象となる各ノードに対して Pod を作成し、作成されるとデフォルトのスケジューラーが引き継ぐ

## Daemon Pod とのコミュニケーション

- push: DaemonSet 内の Pod は他のサービスに対して情報を送信する
- NodeIP、Known port: Pod が NodeIP を介して疎通できるようにするために、DaemonSet 内の Pod は hostport を使用できる
- DNS: endpoints リソースで DaemonSet を探すか、DNS から複数の A レコードを取得
- Service: 同じ Pod セレクターを持つサービスを作成し、いずれかのノード上の Daemon に疎通させるためにそのサービスを使う

# Job

一時的な処理を行うためのリソースで、タスクが完了すると終了する。
スケジュールに沿って job を実行したい場合は Cronjob を使う。

## 並列実行

job に適したタスク

- 非並列 job
- 固定の完了数をもつ並列 job
  - .spec.completions の数の Pod が成功すると完了
- ワークキューを利用した並列 job
  - 他の Pod が終了したかも確認できる

## Pod 失敗ポリシー

.spec.podFailurePolicy フィールドで定義される Pod 失敗ポリシーを使用するとコンテナの終了コードと Pod の条件に基づいてクラスターが Pod の失敗を処理できるようになる
例

- Pod の 1 つがバグを示す終了コードで失敗すると、Job をすぐに終了
- 中断が発生しても Job を完了するように Pod の失敗を無視

## Job の終了とクリーンアップ

Job が完了しても Pod は削除されない
Job を終了させるには、COMMANDO を実行するか、.spec.activeDeadlineSeconds に秒数を設定すること

# 終了したリソースのための TTL コントローラー

TTL コントローラーは実行を終えたリソースのライフタイムを制御するメカニズムがある

## TTL コントローラー

現時点では Job のみサポートしており、リソースが終了したあと指定した TTL の秒数後に削除するようになる

## 注意点

- 一度 TTL 期限がきれると、TTL を伸ばせないことがある
- クラスター内の時間差で誤った時刻に動作する可能性があるので NTP 稼働を必須とする

# Cronjob

繰り返しのスケジュールによって Job を作成する
一度のスケジュール実行であり、2 つのジョブが作成される、または１つも作成されない等の場合がないとは言えないので、ジョブは冪等であるべき

# ReplicationController

水平スケーリングでリソース管理できるレガシーな API で Deployment と ReplicaSet に置き換えられた。
