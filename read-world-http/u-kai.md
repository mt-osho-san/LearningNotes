# Real World HTTP

## 1

- スキーマ、ホスト名、パス、ハッシュはどれがどれか忘れがち
- ヘッダーは仕様上フィールドと呼ぶらしい(知らなかった)
- Referer はブラウザが勝手につけるもの？
  - サーバーサイドじゃないよな

## 2 HTTP/1.0 の世界:基本となる 4 つの要素

- いろんな歴史の話
- いろんな組織があるんだな〜という感じ
- MDN が Mozilla ってのは知っていたけど、Google や MS もコントリビュートしているのは知らなかった
  - 事実上のブラウザの標準みたい
- HTTP/0.9 では HTML 以外を送る想定がなかったり、ステータスを返すこともできなかった
- ヘッダーに X をつけると独自ヘッダーとして定義できるのは知らなかった
  - X-xxxx みたいなやつはよく見かけていたから理由がよくわかった
- ヘッダーの Key 値はライブラリによって正規化の方法が変わるのはなんとなくそんな気がしてたけど実際に知れてよかった
- Content Sniffing はお節介だったってことか
  - X-Content-Type-Options: nosniff はよくみるが、今ではおまじないレベルって感じか
- 3xx 版のステータスはリダイレクトの意味と思っていて、それはあっているけど、もっと抽象的にいうとサーバーからの命令って書いてあって、なるほどなーと感じた
- 999 ステータスコード知らなかった
  - お茶目すぎ
  - 429 でいいやん
- リダイレクトの回数に制限をなくしたがブラウザ側で無限ループを検知して止める責務が追いやられているのは知らなかった
- ヘッダーやクエリは複数同じ key を使ってもいいけど、どうパースされるかはフレームワーク次第はまじで鬱陶しい
  - go は同じ key を複数使うと配列
  - rails は同じ key を複数使うと最後の値だけ評価される
    - 配列にしたいなら `key[]=value1&key[]=value2` みたいにする
      - 若干 rails は違和感
- テキストにマークしてリンクに情報を含めるのはよくやるけど、これは URL フラグメントテキストディレクティブっていうんだ
- 国際化ドメイン名とエンコーディングルール Punycode は初めて知った。こんなことできるんや
- URL にめちゃめちゃクエリとかついているのはバックエンド側で統計とか取っていることがあるから説は面白い
  - そのため、ノイズがないようにバックエンド側は正規 URL を好むってのも面白い
- curl で-d @FILE_NAME でファイルの中身を body として送れるのは知らなかった
- 前の URL の 2000 文字制限のために Elasticsearch は GET で Body を受け取るの知らなかった

  - ただ、前の説明でもあるように GET で Body は送れないようにした方が良いし受け取らないべきだと思う

- ヘッダーではなくフィールドと呼ぶべきだったり、URL や URI が混ざったりと標準化ってやっぱり難しいなと思った

## 3 HTTP/1.0 のセマンティくす:ブラウザの基本機能の裏側

- x-www-form-urlencoded の細かい仕様は忘れたら見返したい
  - 一応 OAuth の仕組みでも使うことがあるので、JSON でのやりとりが主要ではあるものの、知っておくと便利だと感じた
- multipart/form-data はファイルのアップロード時に使うっていう話は知らなかった
- 様々なメタデータを送ることができるので、ファイルデータは multipart/form-data でしか送れないってのは初しり

  - json しかほとんど送らないからここら辺は疎い

- ランダムな区切り文字で複数データを渡せるらしい

- Accept とかのフィールドはコンテントネゴシエーションっていうんだ
- Cookie って GDPR だと基本ユーザーに同意を取らないといけないものだと思ってたけど、厳密に必要なクッキーは別に同意なしでいいのか
- なので、認証情報などは別にユーザーの同意なしに送っていいってことか?
- また、ショッピングカート機能みたいなシステム上必要なものは OK らしい
- Cookie に 4kb の制限(推奨値？)があるのは知らなかった
- Cookie の属性はセキスペでよくやったなーという感想
- いつも思うけど、CORS って必要？
  - 他サイトからのリクエストを防げるのは利点に見えるけど、サイトじゃなくて悪意のあるリクエストを curl とかで送れば良くない？それは CORS 防げないし。と思う
  - また調べよ
- Basic 認証と、Digest 認証は今ではあまり使われないらしい
  - Basic 認証はなんだかんだ GitHub の認証付きリクエストや OAuth のクライアントシークレットフローで使われていると思う
  - あとはエンタープライズ企業の古システムでは使われていて、エンプラ向けにシステム作成するのであればサポートしないといけないってこともある
- 電子署名を使ってサーバーサイドに認証情報を持つのではなく、クッキーだけで認証させるのはなるほどという感じ
  - 署名使えば公開鍵だけで検証できるのいいよなという感じ
- 304 Not Modified はなんとなくみたことあるけど、しっかり理解できた
- 確実にキャッシュしない条件があるのは知らんかったのでこの項目は今後のリファレンスにしたい
  - これで思ったんだけど、通信全部 POST でよくない理論は、キャッシュのことを考えるとよくないかも
  - もし POST でキャッシュしたくなるのであればそれはアプリケーションのロジックで行わないといけなくなる
- ETag は便利そうだと感じた。Expires よりも正確にキャッシュを制御できる気がした
- Cache-Control ヘッダーもよく見るがよくわかってなかったのでみれてよかった
  - サーバーからクライアントへのキャッシュに対する指示
  - public の説明で言う、同一のコンピュータってクライアントのコンピュータ？それともサーバーのコンテンツ？
  - 多分前者っぽい
  - immutable は強気すぎんか？変化しないこととかある？path ごとにコンテンツを変えるものであればなくはないか。ただ非標準
    - no-cache はキャッシュを使わないではなく、更新があるかどうかをサーバに聞きにいくってのはわかりにくい
- キャッシュは便利な一方、ここまでキャッシュに対する条件分岐があると、キャッシュが関係するかもしれない障害対応の切り分けをする際に大変そう
- あと、通信が繋がらない〜＞つながるになった時原因は違うにも関わらずキャッシュのせいにしがちになる気がするし、実際なっている
  - キャッシュの昨日をちゃんと把握して本当にキャッシュの成果どうかは判断できるとかっこいいかも
  - あとは、キャッシュはブラウザだけじゃなく様々なコンポーネントが持ちうる可能性があるのがだるい
- Vary ヘッダーは表示が変わる理由を示すもので、これによって正確にコンテンツをキャッシュさせることができる
  - 検索エンジンのヒントにも使われるらしい
  - Vary があることでクライアント毎に良い検索結果をエンジンが返してくれる感じ
- クローリング時は robots.txt を守りましょう！また、コンテンツを出す側も robots.txt を書いとかないとクローリングされても文句は言えない
- robots.txt はブラックリスト的だが、サイトマップはホワイトリスト的な仕組み

- 感想としては、あまりここら辺は気にせずともアプリケーション作れているのでフレームワークって相当偉大だなと思ったし、アプリケーション作るのであればフレームワークを使うべきだと感じた
- もしくは JSON 返すだけの API サーバーが楽な気もする
- API サーバーであれば API Gateway などのマネージドな仕組みを前段に置いておくといろいろ共通の処理やセキュリティの対策が楽になるので、そういったものを使うべきだと感じた

## 5 HTTP/1.1 のシンタックス:高速化と安全性を求めた拡張

- HTTP/1.1 はめっちゃ長生きしてるな
- Keep-Alive は TCP コネクションを使い回すって感じかな
- HTTP/2 では常時 Keep-Alive でヘッダーに含めてはいけないのは知らなかった
- パイプライニングは不遇

  - 理由的にはうまく実装されていないサーバーが多いからって感じはしたが、並列に送信してもその順番で返さないといけないのは確かに時間節約効果が薄くなりそう
  - HTTP/2 では改良されたストリームに変わっているみたい

- TLS は様々なプロトコルに使える！(これが層に分ける仕組みよな)
- セキュリティ関連の技術は技術選択肢が 1 つだけという状態を禁じる慣例があるのは知らなかった
  - 確かに一つしかなくて、それがやられたら終わりなので理には叶っている
- DHE はいつもわからん。。。知っている値が多すぎて中間者でも乗っ取れるのでは？と思ってしまう
- 公開鍵が共通鍵に比べて 15000 倍も遅いのはびっくりした

  - IT の世界は人間の感覚では全部処理が高速すぎてこういうのちゃんと見ないと知らない間にボトルネックになっているんだろうな〜という気持ち

- X.509 は至る所で利用されているイメージ。ここら辺知っておくのはエンジニアとして重要そう
- Session や Keep-Alive のおかげでだいぶ通信の効率がよくなった
  - 命令セットのレベルで AES を高速化するのはすごい
  - HTTP/2 ではセッション数が最大で 1/6 で、TLS1.3 ではハンドシェイクの数も減っているらしい
- 暗号強度を上げると計算負荷が高まって、DoS に弱くなるってのは確かにと思った
- TLS が守るもの、はまとまっててみんな読むべき笑
- ACME プロトコルは楽で良い。もう少し詳しく知りたい技術の一つ
- EV 証明書のアドレスバー意味ない説は面白い
- PUT,DELETE 追加の話で、HTML のウェブフォームからはこれらのメソッドは利用できないのは知らなかった
- HTTP メソッドはプリミティブな操作だけど、HTTP はドキュメントを扱うさらに上位な API という違いがあるのは確かにそうだなと思った

  - HTTP メソッドを発行した後のことは HTTP では隠蔽して、より抽象度の高い結果を返す、HTTP 自体は一回のやりとりで完結する
  - まあ、ここら辺は 思想の話なので、仕様上そういうものっていうわけではなく、そう作った方がいいよねという話だと思う

- OPTIONS メソッドの役割がそのサーバが利用できるメソッドを知らせるものっていうのは知らなかった
  - CORS の時に Preflight で問い合わせるものっていう認識だった
- 基本は OPTIONS は閉じられているらしい
  - おそらくセキュリティのためかな
- TRACE メソッドはリクエストの内容をそのまま返すようなメソッドが非推奨メソッドらしい

  - XST というクロスサイトトレースの脆弱性があるから
  - XST は TRACE によってクッキー情報などを盗む攻撃
  - Cookie は基本的に HttpOnly や SameSite などの属性によって守れているが、TRACE によって返却されるレスポンス情報はただのバイナリかつ、そこに Cookie 情報が含まれているので、それを盗むことができる
  - おもろい！

- CONNECT で HTTPS 以外は拒否するというくだりはよくわからない

  - 何を強調したい？
  - 前職で Proxy と戦い続けたことを思い出す。。。

- PATCH メソッドはリソースの部分的な更新を行うメソッド

  - PUT はリソース全体を更新するメソッド
  - ここまでちゃんと区別するのは HTTP の思想であるドキュメントに対する操作の抽象度を漏洩してない？？
  - PATCH なのか PUT なのかはアプリケーションの設計次第って感じかな？
  - ただ、メソッドごとにハンドラーを設けておけば、操作が違う場合、同じメソッドないで処理を分岐させるよりもわかりやすいかもしれない
  - あと GPT さんに聞いたら冪等性の話が出てきた
  - PUT は冪等性を担保するべきだが、PATCH は冪等かどうかはわからないとのこと
    - PUT ではマルっと置き換えるが、PATCH では部分的に変更するので、例えばある値をインクリメントするという操作が PATCH には実装でき、冪等性を担保できないみたいな感じ

- HTTP/1.1 からは HTTP 以外へのプロトコルのアップグレードができるらしい
- クライアントからでもサーバからでもアップグレードを要請できる
- 種類は以下

  - HTTP から TLS を使った安全な通信へのアップグレード
    - これは RFC2817 で説明されているもので、これをやってもセキュリティ敵に守られない可能性があるみたい
    - HTTPS とは違うってこと？
      - 暗号化をするのが HTTP の前か後かみたい。このアップグレードは HTTP をした後にアップグレード
    - 今は TLS そのものが持つハンドシェイク時のプロトコル選択(ALPN)を使うことが推奨されており、HTTP/2 ではこのプロトコルのアップグレード機能が削除されているらしい
  - HTTP から WebSocket へのアップグレード
  - HTTP から HTTP/2 へのアップグレード

- クライアントからのアップグレード要請は Upgrade,Connection ヘッダーを使う
- Upgrade 可能か知るためにまず OPTIONS メソッドで確認する
  - でも OPTIONS は基本閉じられているんじゃなかったけ？
  - 確認できなかったら Upgrade はやめるんかな？
- サーバ側からはステータスレコード 426 をつけるらしい
- TLS 通信への切り替えはリダイレクト機能とか使おうねとのこと

- バーチャルホストは便利
- チャンク送信の Body 部に対して詳しくなれた
  - 最後に 0 だけのチャンクを送ることで終了を示す
- ブラウザからアップロード方向の通信でチャンク送信を使えないのはなんで？
  - 一応頑張ればできるっぽいけど、標準の方法じゃないからクライアント、サーバともに頑張る必要はありそう
- Trailer で指定したヘッダは最後に送信できる

- Expect: 100-continue は知らなかった。100 Continue が返ってきたら受け入れ可能とのこと
- 無理だったら 417 Expection Failed とのこと
- 実は curl はよしなにやってくれていたとのこと
- データ URI スキームによってデータそのものを表現できるので、画像をそのままデータとしてツッコで、解釈できるようにするものらしい？
  - 画像をそのままデータとして埋め込むことで、リクエストを減らすことができる
  - ただ、一回のデータ総量は多くなるよな？というところ
  - これ使わない方が画像以外はすぐにレンダリングされてそれはそれでいい場合がある気がする

## 6 HTTP/1.1 のセマンティクス:広がる HTTP の用途

- Content-Disposition フィールドでファイルをダウンロードさせることができるのは知らなかった
  - 拡張子でダウンロードさせるかどうか判断させていると思っていた
- Accept-Ranges を使って範囲指定ダウンロードできるのは知らなかった
- If-Range で条件を指定して、ファイルの更新を検知できるのはなるほどって感じ
- Ranges フィールドを使って複数範囲の指定も可能だが、このユースケースがよくわからん
- Comet は聞いたことあったけど、詳しくは知らなかった
  - ロングポーリングな仕組みなので、最近ではあまり使われていない感じかな
- Geo-Location の仕組みで、、Wi-Fi から位置測定をするのは知らなかった。
  - データベースを用意するという結構泥臭いことがやられているというのが印象的
  - もっとスマートなやり方かと思っていた
- ブラウザに Geolocation API が用意されているってのも知らなかったけど、これが位置情報を特定するやつって思えば当たり前か

- GeoIP も地道にデータベースを用意しているみたい

  - IP ってすぐに変わったりしないもんなのか？
  - データベースを頻繁に更新しないと正しい結果にならないってあったが、大変そうだし、誰が責務を持ってちゃんと更新する、もしくはさせるのかな？

- X-Powered-By と Server はシステム名を返すものだが、著者の言うとおり、今後はそこまで利用したいとは思わない
- XML-RPC は相当だるそう
- XML-RPC を公開しているサーバーは多少のエラーがあってもステータスには基本的に 200 を返すことになっているは意味わからん

  - 考え方としては、設定の問題かサーバーコードの問題かを切り分けやすくするためらしいが、そこはログとかちゃんと取れば良いのでは？と言う感じ。
  - 自分のシステムなのであれば、内部を調べればいいと思うし

- SOAP はエンタープライズ志向の会社が仕様策定に関わっていて、こってりしているというのは面白い

  - 標準をエンプラにやらせると色々大変な目に遭いそうな予感笑

- JSON-RPC は SOAP に比べてシンプルだが、どちらにせよ存在意義がよくわからん
  - RPC という標準を作りたかった？
  - まあ確かに json-rpc に合わせておけばデータの取得は簡単になるのかな？
  - ただ、全く流行っている気がしない
  - method を path とかで表現するのではなく json の中で指定するのが json-rpc の特徴見たい
- WebDAV はネスペかなんかで聞いたことあるけどよくわからん

  - ファイルをネットワークごしで操作するみたいな感じだけど、今はアプリケーションごとにリッチな仕組みを実装している感じ
  - ネットワークに繋がっていることが前提なのは相当辛い気がする

- SAML ってだいぶ古い仕組みだけど今の SaaS でも実装されているあたり、これもエンタープライズ企業の影響がある気がする

  - あとは HTTP POST バインディングで Idp と SP の直接通信がない通信方式でプライベートな認証サーバを使ってインターネット上のサービス認証ができることがエンプラにも好かれそう
    - でもこれって、ブラウザを経由した間接的なやりとりであれば、OIDC の Implicit Flow でもできるような気がする

- OAuth の Device Code Grant は知らなかった。キーボードなしとか書いてあるけど、結局別の機器でユーザーコードを入力するんか
  - デバイス側は入力せずともよくて、別端末での認証が終了したかどうかをデバイスが認証エンドポイントにポーリングする形で問い合わせを行い、アクセストークンを取得するやり方みたい
- PKCE で今ではフロントエンドだけでもセキュアに Authorization Code が利用可能になっている
- イタリア政府が SAML 非推奨にして OIDC を義務化しているのはすげー
- OIDC ではユーザー情報を取得するプロフィール API の使用が標準化までされているのは知らなかった
  - 同じ方法でユーザー情報を取得できるのは便利
- Hybrid Flow は ID トークンにアクセストークンや認可コードが改竄されてないかを検知するためのハッシュ値が格納されており、リクエストの改ざんを見抜けるようになっているため、Financial Grade API というものでは必須になっているみたい
- JWT は検証ができて仕舞えばペイロードに入っている情報だけで認証サービスへの問い合わせいらずでユーザー情報が取得でき、便利
- JWT の注意点として、このような検証を行わなくても値を利用できてしまうことと書いてあったがわかるという感じ
  - やっぱりなんとなくではなく、しっかり OIDC の使用などは理解しておくべし
  - まあ、ライブラリ使っていれば、勝手に検証までしてくれて、検証失敗したらエラーとかにしてくれそうだけど
- サーバー側でトークン無効にするのが難しいとのこと
- CBOR は初めて知った。JSON の代替になるんかな？それとも全く責務が違うんかな？
- JWT は sub に含まれるキーだけが変化しない安定したキー
- Capabilities URLs は前職で少し齧った。

  - W3C のドキュメントでは、機密情報を公開するのはよくないという感じ
  - URL が総当たりで見つかるというよりか、誰かが漏らすという感じ

- 短縮 URL は大元の URL を隠すからセキュリティリスクがあるってのはなるほどなという感じ

## 7

- openssl コマンドを分けて実行しないと AWS IAM などが受け付けられないのはなぜ？
- サーバーの証明書を作成するところ、証明書を自分の秘密鍵で署名して作成じゃなくね?
  - CSR は自分の秘密鍵で署名して、証明書は CA の秘密鍵で署名するのでは?
- openssl コマンドで作成した server 証明書がずっと invalid ECDSA parameters で怒られる。。。
  - public key に問題がありそうなところまでわかったが、どうやって修正するかがわからない
- 高レベルから低レベルまで go は標準で http のクライアントや通信のクライアントが提供されているのまじで便利
  - 他言語でここまで標準で使いやすくラップされているものを用意しているのは少ないのでは？
- デフォルトでチャンクを使えるのは便利
  - Post で 2048 バイト以上のファイルを ContentLength なしで送信するとチャンク形式なのは知らなかった
- chunked の server の実装は NewResponseController で返してあげずとも、`w.(http.Fluster).Flush()`で型キャストしてあげればいけた
  - キャストがめんどくさいから、NewResponseController 使っているのか？
- RPC の良さがわからん。。。

## 8 HTTP/2, HTTP/3 のシンタックスプロトコルの再定義

- バイナリプロトコルへのへのうや TCP レイヤーとの重複操作をまとめるなどをしているが、HTTP/2 や HTTP/3 を利用するアプリケーションには大きな違いはないらしい

  - ブラウザなどのクライアント側が頑張って変換してくれているっぽい

- HTTP/3 は暗号化が必須で、暗号化が必要ないところでは HTTP/1.1 が使えるらしい
  - DC では非暗号化って書いてあったけど、今後の内部通信は mTLS が普通になる気もする
  - ロードバランサーによる SSL 終端でパフォーマンス高めたいなら HTTP/1.1 は確かに
- HTTP2 も HTTP3 も通信品質が高くない環境や Wifi とモバイル回線が切り替わるような状況を考えているものらしい
  - HTTP1.1 もそこそこ早いとのこと
- ALPN を使ってプロトコルのバージョンを変えられるが、TLS の上での話なので、UDP の HTTP3 への選択はできない
  - HTTP Alternative Services で HTTP/3 へ再接続をさせるらしい
  - ここまでして繋げるほど高速なんか？
- DNS の HTTPS レコードを使えば DNS の段階でわかる

  - こんなレコードがあるとは。。。

- HTTP2 は HTTP のセマンティクスは変わらないので、RFC9113 にはバイナリ通信のフォーマットの説明しかないみたい
- SPDY は HTTP2 の前身で、Google が開発したもの
  - 全部 Google やん
- 30％から 3 倍以上のパフォーマンス向上があると報告されていたらしく、並列アクセスのブロッキングが少なくなる特徴があるので、小さなファイルをたくさん転送するほど高速化するみたい
- コラムに Google は大規模ウェブトラフィックとブラウザどちらも持っているので、独自プロトコルを試せるのが強いとあり、なるほどなるほどという感じ
  - IT の発展という意味で誰が Google に勝つんや笑
- ファイルを細かく分割して並列に送るのも通信回数が多くなったりパケットの無駄も増えるので一気に送ることと一長一短っぽい
- HTTP2 の大きな変更点はストリームを使ったバイナリデータの多重化とフィールドの圧縮
- HTTP2 は一つの TCP コネクションの中で仮想のストリームを何本も作ることができるらしい
  - HTTP2 のデータはフレームといらしい
- Stream Identifier がストリームの ID で 0 は予約済み、奇数はクライアントからサーバーへ、偶数はサーバーからクライアントへのストリーム
- データは減ってフィールドとコンテンツのみ
- バイナリとかテキストっていうのはバイナリでデータの表現やデータ長などを知らせる感じで、テキストは改行とかでデータの区切りを教える感じ？
- テキストだと最後まで一文字ずつ読まないとダメだとけど、バイナリであればフレームサイズも入っているし、高速にリクエストもできるらしい
  - テキストでもフレームサイズみたいなもの入れれば良いのでは？？Content-Length とかはそれではないの？
  - バイナリだから早いのはちょいとわからん。データ構造が良いのでは？テキストベースって言っても送信される時はバイナリでしょ？
- やっぱり、コンテンツに関しては Content-Length で決まるからあまり変わらないとのこと
- ただ複数リクエストの場合は違くて、HTTP1.1 では 1 リクエストの最中に他のリクエストを処理できないが、HTTP2 ではストリームを使って並列処理ができる
  - フレームは独立しているから途中に他のフレームが挟まって OK ってあるけど、意味のある形にするにはやっぱり全部のフレームが揃ってないと無理なのでは？
  - まあでも少しでもデータを取って来れるのはありがたいのか？でも例えば、画像の真ん中のデータだけ取って来れなかったとかだと、そもそもバイナリの復元ができずに意味がないのでは？
  - そういうことではない？
- HTTP2 のフローコントロールは通信速度に差がある機器同士の通信をスムーズにするためのもの

  - 通信速度が遅い方に合わせて通信を行うように Windowsize を調整するみたい

- プリロードを使って CSS や JS を先に読み込めるみたい。その後の動作や優先度も決められる模様
- 圧縮の方法初めて知った
  ```rust
  // こんな感じ？
  struct Dict {
      inner: HashMap<String,String>,
  }
  struct DictKeys(Vec<String>);
  ```
- HPACK ってのでは、定義済みの静的テーブルを持っているらしい.HTTP のフィールドは決まったものがほとんどだから最初に決められるよねってことみたい

  - これ X~みたいなやつはどうするんだろ？っていうのと静的テーブルは誰が持っているの？送信側の全てのクライアントが持っている？
  - 一応動的テーブルもあるみたいで、同じコネクション内のデータは貯めていくみたい

- HTTP3 も Google さんの QUIC...
- UDP にしていることと、TCP と HTTP で重なっていた処理を見直していることが特徴らしい
  - TCP は上の層を知らないので全てに対して順序整理などをするが、QUIC ではそこを統合して、本当にアプリケーションがほしいものだけを整理している感じなのかな？
- 実は昔から Google への半分ぐらいの通信は QUIC にされていたらしい笑
- QUIC は HTTP に限らないプロトコルなので HTTP over QUIC という名前もあるらしく、これが HTTP3 とのこと
- 現代のようなネットワーク経路が変わるような状況で、QUIC は適していて、再接続に伴うラグが発生しにくいようにできているみたい
- コネクション ID というものでコネクションを判断しており、ネットワーク経路に依存していないため、接続を維持できるみたい
- RFC 9000 が勉強におすすめらしい
- HTTP2 と違って QUIC トランスポート側と HTTP3 側で別れており、HTTP3 側はシンプル
- QUIC トランスポートはストリーム内しか順番保証がないので HPACK が使えないってあるけど、HTTP2 もストリーム間は順番保証ないよね？なんで HPACK 使えてたの?
- 順番に依存せずに済むので、ストリームを止めなくても済んで HoL ブロッキングが起きにくいらしい
- QPACK と HPACK の説明はなんだか難しいこと言っているのであとで TODO
- 複雑すぎたのやつも TODO
- WebSocket はステートフルってやつ、実装や要件次第では？とも思った

  - ただ、負荷分散している場合じゃねぇってことがあって、サーバーのインメモリでやらざるをえないケースもあるんだなーと初めて知った

- WebTransport という WebSocket の後継があるらしい
- HTTP3 や QUIC 上に構築されるものみたいで、WebSocket のように複数セッションを張るときにストリームを利用できて処理コストが低いし、UDP ベースなのでリアルタイム通信に向いているらしい

  - HoL ブロッキングも起こしにくいとこのと
    - これは TCP,UDP の違いによるっぽい気がする
  - 再送処理の保証がないモードもあるみたい

- WebRTC はブラウザ／サーバー間だけではなく、ブラウザ／ブラウザ間の P2P 通信も可能
- NAT 超えとかもできるらしい
- ファイヤーウォール越しで利用可能とかプロキシ経由でも利用可能にする必要があるとか、物作りって大変やな笑
- STUN とか TURN とかまじで知らん。ネスペとってもわからんこととしかない笑

  - ただ、VoIP とか同じような仕組みだった気もする
    - NAT が頑張ってプライベート IP のクライアントと繋げてくれるみたいな
  - 電話交換機みたいに中継してくれる奴がいての話っぽいな
  - TURN サーバーはあると嬉しい、っていうところなんで？って感じでなんもわからん笑

- HTTP ウェブプッシュは今の所ブラウザを作成している会社の機能って感じ？？
- service worker はサーバーと常に通信のコネクションを貼っているって感じなのかな？
  - そうだとすると、いろいろ負担にならんか？
  - まあでもメールとかもその類か
- ブラウザなどに通知を飛ばしたい場合は実装していくことになるんだろうな
  - ただ、ブラウザごとにプッシュサーバーがあるみたいなので、ブラウザごとに作成しなくてはいけない雰囲気。。。

## 9 HTTP/2 時代の新しいユースケース

- デバイスピクセルレシオは初めて知った
- レスポンシブデザインのために meta タグでブラウザの拡大縮小をやめてもらうことが必要なのは知らなかった
- srcset でどうやって適切なファイルを選択するんだ？
- アルファチャンネルとか可逆非可逆とか何がって感じ
- HEIF は圧縮力がすごいけどブラウザ側の準備がまだまだらしい

  - 何がそんなに難しいのか？

- セマンティックウェブや schema.org の責務がいまいちわからん

  - 人間だけでなく、機械も情報を理解できるようにするため、みたいなことを GPT に言われた
  - これにより、有益な分析や検索ができるようになるみたいなことみたい

- shcema.org はウェブ上のデータの構造化を推進するものらしい
- マイクロデータによって web ページにメタデータを埋め込むことができるらしい
- JSON-LD は Google が推奨しているメタデータの記述
- vCard や iCalender はなんとなくみたことあるけど、Apple の機能だと思っていた
  - こんな標準があるとは思わなかった
  - 確かに、google の予定もちゃんと Mac のカレンダーとかに入れられるのはこういう標準があるからか
- オープングラフプロトコルも初めて知ったが、いつも出現しているやつ！ってなった

  - 受け取る側が賢いのかと思っていたが、標準化されているのね

- QR コードにも色々なモードがあるのは知らなかった
  - 最悪 50％が欠損してても修正できるモードはかなりすごいなと言うところ
- 様々な仕様があるので、一気にあるアプリケーションに跳べたりするのは便利だなと思った
- リンクからアプリケーションに飛ぶ仕様は Deep Link というらしい
- HLS は HTTP を利用しているので、インフラ面は比較的簡単に利用できる
- 実はストリーミングではなく、逐次ダウンロードなので 30 秒ほどの遅延があるみたい
- しかもサポートされているブラウザが少ないらしい
- MPEG-DASH は別の標準化のもので、HTTP によるダウンロードをは変わらないが、動的にビットレートを切り替えられるらしい
- MPEG-DASH はブラウザが頑張るんじゃなくて、dash.js っていライブラリが頑張ってくれうるので、使いやすい
- CMAF っていうのは MPEG-DASH と HLS のマニフェストをそのまま利用するものらしいが、遅延を減らすための工夫をしているもの
- HLS はセグメントサイズに比例した遅延が発生するが、CMAF はもっと小さいチャンクを利用していて、遅延を大幅に減らしているらしい
  - 名前変えただけでは？もっと何かしている気もする

## 11: クライアント視点で見る RESTful API

- 2010 時点で公開されている API の 74％が RESTful API だったらしい
- REST は HTTP のルールになるべく従うことで HTTP のメリットを享受しやすくなることを目的としている
- URL はリソースの階層を表したパスになっていて、名詞のみで構成されているっていうのは規則を守るのに大事かも
  - /getContent とかやりがち
- HTTP メソッドによってリソースの取得や更新を操作すること
- トランザクションは存在しないっていうのはあくまでステートレスってことかな？
- RPC は URL は一つで、リクエスト時にサービス名とメソッド名をパラーメータで渡して何をするのかを指定する
- なんで Dropbox は RESTful API を使わなかったのか？
  - なんか理由があるのかな？
- メソッドは全部 POST らしい
  - 良い意味で HTTP に依存してないとも言えるのかな？
- RPC の意義がわからなかったけど、よく考えると HTTP よりもやりたいことの意味はわかりやすいかも？？
  - RESTful は前述のように HTTP の上に載っている感じで、HTTP を知っていないと少しわかりにくいかも
  - 自分たちのアプリケーションに HTTP の関心ごとが入ってくる感じ(言い過ぎ？)
- パスを使ってリソースを指定するっていうの結構難しいし、3 階層ぐらいまでしか許さずに、API を分ける思想であればアンダーフェッチみたいなことは確かに起きるな
- GraphQL はここら辺を解決する感じか
  - REST を復習したら少し GraphQL の理解が深まった気がする
- JSON-RPC は複数のリクエストを配列にまとめて同時に発行できるの確かになと思った
- アトミックな処理をサポートしているやつもあるらしく、確かに複数リクエストで一つのことをしたいのであればアトミック性を担保しないといけないか
- でもアトミック性を担保したいのであれば、それは一つの API であるべきな気もする
- Content-Type multipart/mixed で複数のリクエストを送信できるのは知らなかった
- HATEOAS の理想は API からリンクを辿って目的のリソースにたどり着くことができること
- MS の API は REST を名乗るなと怒られたのウケる笑
- そのぐらい REST は 厳格で、HTTP の上に載っているものであるということなのかな？
- REST-ish な API は REST に似ているが、REST の原則を完全には満たしていないもの
- オーバーロード POST というものもあるらしい
- Google の API デザインガイドで紹介されているパスの後ろに:メソッド名を追加する方法は REST ではない？
- トークンの有用性として、ユーザー ID と Password よりも優れているという説明でトークンはいつでも無効化できるとあり、なるほどと思った
  - UserId とか Password だと User 自体の無効化が必要で、token だとトークンだけの無効化なので、なるほどなと思った
- conf.TokenSource()使って OAuth トークン取得すると RefreshToken を使ってトークンを取得してくれるのは便利
  - 最近 OAuth の実装をしなくてはいけなくなったのでぜひ使いたいと思った
  - バッチで RefreshToken 使ってトークンを有効化しようと思っていたが、その必要はなさそうやな
- RESTful ってまだ新しい企画とかが出ようとしているんだなと思った
- Sunset ヘッドは嬉しいかも
- エラーを返す時につかう JSON フォーマットもあれば嬉しい
  - エラーの時どのような実装にしようか迷わずに済むから
  - 同様の理由でヘルスチェックの結果も JSON フォーマットであると嬉しいな
