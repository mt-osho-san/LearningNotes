# Real World HTTP

## 1

- スキーマ、ホスト名、パス、ハッシュはどれがどれか忘れがち
- ヘッダーは仕様上フィールドと呼ぶらしい(知らなかった)
- Referer はブラウザが勝手につけるもの？
  - サーバーサイドじゃないよな

## 2 HTTP/1.0 の世界:基本となる 4 つの要素

- いろんな歴史の話
- いろんな組織があるんだな〜という感じ
- MDN が Mozilla ってのは知っていたけど、Google や MS もコントリビュートしているのは知らなかった
  - 事実上のブラウザの標準みたい
- HTTP/0.9 では HTML 以外を送る想定がなかったり、ステータスを返すこともできなかった
- ヘッダーに X をつけると独自ヘッダーとして定義できるのは知らなかった
  - X-xxxx みたいなやつはよく見かけていたから理由がよくわかった
- ヘッダーの Key 値はライブラリによって正規化の方法が変わるのはなんとなくそんな気がしてたけど実際に知れてよかった
- Content Sniffing はお節介だったってことか
  - X-Content-Type-Options: nosniff はよくみるが、今ではおまじないレベルって感じか
- 3xx 版のステータスはリダイレクトの意味と思っていて、それはあっているけど、もっと抽象的にいうとサーバーからの命令って書いてあって、なるほどなーと感じた
- 999 ステータスコード知らなかった
  - お茶目すぎ
  - 429 でいいやん
- リダイレクトの回数に制限をなくしたがブラウザ側で無限ループを検知して止める責務が追いやられているのは知らなかった
- ヘッダーやクエリは複数同じ key を使ってもいいけど、どうパースされるかはフレームワーク次第はまじで鬱陶しい
  - go は同じ key を複数使うと配列
  - rails は同じ key を複数使うと最後の値だけ評価される
    - 配列にしたいなら `key[]=value1&key[]=value2` みたいにする
      - 若干 rails は違和感
- テキストにマークしてリンクに情報を含めるのはよくやるけど、これは URL フラグメントテキストディレクティブっていうんだ
- 国際化ドメイン名とエンコーディングルール Punycode は初めて知った。こんなことできるんや
- URL にめちゃめちゃクエリとかついているのはバックエンド側で統計とか取っていることがあるから説は面白い
  - そのため、ノイズがないようにバックエンド側は正規 URL を好むってのも面白い
- curl で-d @FILE_NAME でファイルの中身を body として送れるのは知らなかった
- 前の URL の 2000 文字制限のために Elasticsearch は GET で Body を受け取るの知らなかった

  - ただ、前の説明でもあるように GET で Body は送れないようにした方が良いし受け取らないべきだと思う

- ヘッダーではなくフィールドと呼ぶべきだったり、URL や URI が混ざったりと標準化ってやっぱり難しいなと思った

## 3 HTTP/1.0 のセマンティくす:ブラウザの基本機能の裏側

- x-www-form-urlencoded の細かい仕様は忘れたら見返したい
  - 一応 OAuth の仕組みでも使うことがあるので、JSON でのやりとりが主要ではあるものの、知っておくと便利だと感じた
- multipart/form-data はファイルのアップロード時に使うっていう話は知らなかった
- 様々なメタデータを送ることができるので、ファイルデータは multipart/form-data でしか送れないってのは初しり

  - json しかほとんど送らないからここら辺は疎い

- ランダムな区切り文字で複数データを渡せるらしい

- Accept とかのフィールドはコンテントネゴシエーションっていうんだ
- Cookie って GDPR だと基本ユーザーに同意を取らないといけないものだと思ってたけど、厳密に必要なクッキーは別に同意なしでいいのか
- なので、認証情報などは別にユーザーの同意なしに送っていいってことか?
- また、ショッピングカート機能みたいなシステム上必要なものは OK らしい
- Cookie に 4kb の制限(推奨値？)があるのは知らなかった
- Cookie の属性はセキスペでよくやったなーという感想
- いつも思うけど、CORS って必要？
  - 他サイトからのリクエストを防げるのは利点に見えるけど、サイトじゃなくて悪意のあるリクエストを curl とかで送れば良くない？それは CORS 防げないし。と思う
  - また調べよ
- Basic 認証と、Digest 認証は今ではあまり使われないらしい
  - Basic 認証はなんだかんだ GitHub の認証付きリクエストや OAuth のクライアントシークレットフローで使われていると思う
  - あとはエンタープライズ企業の古システムでは使われていて、エンプラ向けにシステム作成するのであればサポートしないといけないってこともある
- 電子署名を使ってサーバーサイドに認証情報を持つのではなく、クッキーだけで認証させるのはなるほどという感じ
  - 署名使えば公開鍵だけで検証できるのいいよなという感じ
- 304 Not Modified はなんとなくみたことあるけど、しっかり理解できた
- 確実にキャッシュしない条件があるのは知らんかったのでこの項目は今後のリファレンスにしたい
  - これで思ったんだけど、通信全部 POST でよくない理論は、キャッシュのことを考えるとよくないかも
  - もし POST でキャッシュしたくなるのであればそれはアプリケーションのロジックで行わないといけなくなる
- ETag は便利そうだと感じた。Expires よりも正確にキャッシュを制御できる気がした
- Cache-Control ヘッダーもよく見るがよくわかってなかったのでみれてよかった
  - サーバーからクライアントへのキャッシュに対する指示
  - public の説明で言う、同一のコンピュータってクライアントのコンピュータ？それともサーバーのコンテンツ？
  - 多分前者っぽい
  - immutable は強気すぎんか？変化しないこととかある？path ごとにコンテンツを変えるものであればなくはないか。ただ非標準
    - no-cache はキャッシュを使わないではなく、更新があるかどうかをサーバに聞きにいくってのはわかりにくい
- キャッシュは便利な一方、ここまでキャッシュに対する条件分岐があると、キャッシュが関係するかもしれない障害対応の切り分けをする際に大変そう
- あと、通信が繋がらない〜＞つながるになった時原因は違うにも関わらずキャッシュのせいにしがちになる気がするし、実際なっている
  - キャッシュの昨日をちゃんと把握して本当にキャッシュの成果どうかは判断できるとかっこいいかも
  - あとは、キャッシュはブラウザだけじゃなく様々なコンポーネントが持ちうる可能性があるのがだるい
- Vary ヘッダーは表示が変わる理由を示すもので、これによって正確にコンテンツをキャッシュさせることができる
  - 検索エンジンのヒントにも使われるらしい
  - Vary があることでクライアント毎に良い検索結果をエンジンが返してくれる感じ
- クローリング時は robots.txt を守りましょう！また、コンテンツを出す側も robots.txt を書いとかないとクローリングされても文句は言えない
- robots.txt はブラックリスト的だが、サイトマップはホワイトリスト的な仕組み

- 感想としては、あまりここら辺は気にせずともアプリケーション作れているのでフレームワークって相当偉大だなと思ったし、アプリケーション作るのであればフレームワークを使うべきだと感じた
- もしくは JSON 返すだけの API サーバーが楽な気もする
- API サーバーであれば API Gateway などのマネージドな仕組みを前段に置いておくといろいろ共通の処理やセキュリティの対策が楽になるので、そういったものを使うべきだと感じた
